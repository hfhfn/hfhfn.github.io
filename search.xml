<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo踩坑</title>
    <url>/2025/03/06/Hexo%E8%B8%A9%E5%9D%91/</url>
    <content><![CDATA[<hr>
<h3 id="一-在-Hexo-博客中无法渲染数学公式，通常是因为-Hexo-默认不支持-LaTeX-公式渲染。你需要安装插件并配置-Hexo-以支持数学公式。以下是解决方案：">一. 在 <strong>Hexo</strong> 博客中无法渲染数学公式，通常是因为 Hexo 默认不支持 LaTeX 公式渲染。你需要安装插件并配置 Hexo 以支持数学公式。以下是解决方案：</h3>
<hr>
<span id="more"></span>
<h4 id="方法-1：使用-hexo-renderer-markdown-it-和-markdown-it-katex">方法 1：使用 <code>hexo-renderer-markdown-it</code> 和 <code>markdown-it-katex</code></h4>
<ol>
<li>
<p><strong>卸载默认的 Markdown 渲染器</strong><br>
Hexo 默认使用 <code>hexo-renderer-marked</code>，它不支持 LaTeX 公式。你需要先卸载它：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>安装 <code>hexo-renderer-markdown-it</code></strong><br>
这是一个更强大的 Markdown 渲染器，支持插件扩展：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-markdown-it --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>安装 <code>markdown-it-katex</code> 插件</strong><br>
这个插件用于支持 LaTeX 公式渲染：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install markdown-it-katex --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>配置 <code>_config.yml</code></strong><br>
在 Hexo 的配置文件 <code>_config.yml</code> 中添加以下内容：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">markdown:</span></span><br><span class="line">  <span class="attr">render:</span></span><br><span class="line">    <span class="attr">html:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">xhtmlOut:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">breaks:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">linkify:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">typographer:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">quotes:</span> <span class="string">&#x27;“”‘’&#x27;</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-katex</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>引入 KaTeX 的 CSS</strong><br>
在你的主题文件（例如 <code>themes/your-theme/layout/_partial/head.ejs</code>）中，添加以下代码以引入 KaTeX 的 CSS 文件：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>重启 Hexo 服务器</strong><br>
重新启动 Hexo 服务器，查看公式是否正常渲染：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo server</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h4 id="方法-2：使用-hexo-renderer-pandoc（推荐）">方法 2：使用 <code>hexo-renderer-pandoc</code>（推荐）</h4>
<p>如果你需要更强大的数学公式支持（如复杂的 LaTeX 公式），可以使用 <code>hexo-renderer-pandoc</code>。</p>
<ol>
<li>
<p><strong>卸载默认的 Markdown 渲染器</strong><br>
卸载 Hexo 默认的渲染器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>安装 <code>hexo-renderer-pandoc</code></strong><br>
安装 Pandoc 渲染器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>安装 Pandoc</strong><br>
你需要在本机安装 Pandoc。根据你的操作系统选择安装方式：</p>
<ul>
<li><strong>Windows</strong>: 下载并安装 <a href="https://pandoc.org/installing.html">Pandoc</a>。</li>
<li><strong>macOS</strong>: 使用 Homebrew 安装：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install pandoc</span><br></pre></td></tr></table></figure>
</li>
<li><strong>Linux</strong>: 使用包管理器安装，例如：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install pandoc</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p><strong>配置 <code>_config.yml</code></strong><br>
在 Hexo 的配置文件 <code>_config.yml</code> 中添加以下内容：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">markdown:</span></span><br><span class="line">  <span class="attr">render:</span></span><br><span class="line">    <span class="attr">html:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">xhtmlOut:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">breaks:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">linkify:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">typographer:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">quotes:</span> <span class="string">&#x27;“”‘’&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>重启 Hexo 服务器</strong><br>
重新启动 Hexo 服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo server</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h4 id="方法-3：使用-hexo-filter-mathjax（简单但性能较差）">方法 3：使用 <code>hexo-filter-mathjax</code>（简单但性能较差）</h4>
<p>如果你不想折腾，可以使用 <code>hexo-filter-mathjax</code>，这是一个简单的插件，但性能较差。</p>
<ol>
<li>
<p><strong>安装插件</strong><br>
安装 <code>hexo-filter-mathjax</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-filter-mathjax --save</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>配置 <code>_config.yml</code></strong><br>
在 Hexo 的配置文件 <code>_config.yml</code> 中添加以下内容：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>重启 Hexo 服务器</strong><br>
重新启动 Hexo 服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo server</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h4 id="测试公式">测试公式</h4>
<p>在你的博客文章中，使用以下语法测试公式：</p>
<ul>
<li>
<p>行内公式：<code>$E = mc^2$</code><br>
效果：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo>=</mo><mi>m</mi><msup><mi>c</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E = mc^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord"><span class="mord mathit">c</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></p>
</li>
<li>
<p>块级公式：<code>$$E = mc^2$$</code><br>
效果：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo>=</mo><mi>m</mi><msup><mi>c</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E = mc^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8641079999999999em;"></span><span class="strut bottom" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord"><span class="mord mathit">c</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p>
</li>
</ul>
<hr>
<h4 id="总结">总结</h4>
<ul>
<li>如果你需要简单的公式支持，推荐使用 <strong>方法 1</strong>（<code>hexo-renderer-markdown-it</code> + <code>markdown-it-katex</code>）。</li>
<li>如果你需要更强大的公式支持，推荐使用 <strong>方法 2</strong>（<code>hexo-renderer-pandoc</code>）。</li>
<li>如果你不想折腾，可以使用 <strong>方法 3</strong>（<code>hexo-filter-mathjax</code>），但性能较差。</li>
</ul>
<h3 id="二-Hexo-Admin后台新建posts，一直显示loading">二. Hexo Admin后台新建posts，一直显示loading</h3>
<p>从错误信息来看，问题是由于 <code>js-yaml</code> 库的版本升级导致的。在 <code>js-yaml</code> 4.0.0 及以上版本中，<code>yaml.safeLoad</code> 方法已被移除，建议使用 <code>yaml.load</code> 方法代替。Hexo Admin 插件可能还在使用旧的 <code>yaml.safeLoad</code> 方法，因此导致了错误。</p>
<p>以下是解决该问题的步骤：</p>
<hr>
<h4 id="1-修改-Hexo-Admin-源码">1. <strong>修改 Hexo Admin 源码</strong></h4>
<ul>
<li>找到 Hexo Admin 插件中使用 <code>yaml.safeLoad</code> 的地方，将其替换为 <code>yaml.load</code>。</li>
<li>根据错误日志，问题出现在 <code>D:\AIGC\my_blog\node_modules\.pnpm\hexo-admin@2.3.0\node_modules\hexo-admin\api.js</code> 文件的第 44 行。</li>
<li>打开该文件，找到类似以下代码：<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> settings = yaml.<span class="title function_">safeLoad</span>(fs.<span class="title function_">readFileSync</span>(settingsPath, <span class="string">&#x27;utf8&#x27;</span>));</span><br></pre></td></tr></table></figure>
</li>
<li>将其修改为：<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> settings = yaml.<span class="title function_">load</span>(fs.<span class="title function_">readFileSync</span>(settingsPath, <span class="string">&#x27;utf8&#x27;</span>));</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="2-降级-js-yaml-版本">2. <strong>降级 <code>js-yaml</code> 版本</strong></h4>
<ul>
<li>如果你不想修改源码，可以尝试将 <code>js-yaml</code> 降级到 3.x 版本，该版本仍然支持 <code>yaml.safeLoad</code>。</li>
<li>运行以下命令：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install js-yaml@3.14.1 --save</span><br></pre></td></tr></table></figure>
</li>
<li>然后重新启动 Hexo 服务器。</li>
</ul>
<hr>
<h4 id="3-更新-Hexo-Admin-插件">3. <strong>更新 Hexo Admin 插件</strong></h4>
<ul>
<li>检查是否有新版本的 Hexo Admin 插件，可能已经修复了该问题。</li>
<li>运行以下命令更新插件：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-admin@latest --save</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="4-检查-Hexo-和插件兼容性">4. <strong>检查 Hexo 和插件兼容性</strong></h4>
<ul>
<li>确保 Hexo 和 Hexo Admin 插件的版本兼容。</li>
<li>查看 Hexo Admin 的官方文档或 GitHub 仓库，确认支持的 Hexo 版本。</li>
</ul>
<hr>
<h4 id="5-清理缓存并重启">5. <strong>清理缓存并重启</strong></h4>
<ul>
<li>清理 Hexo 缓存并重新生成静态文件：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
</li>
<li>重新启动 Hexo 服务器：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="6-检查-YAML-文件格式">6. <strong>检查 YAML 文件格式</strong></h4>
<ul>
<li>确保 Hexo Admin 使用的 YAML 配置文件格式正确，没有语法错误。</li>
<li>可以使用在线 YAML 校验工具（如 <a href="https://www.yamllint.com/">YAML Lint</a>）检查配置文件。</li>
</ul>
<hr>
<h4 id="7-全局替换-yaml-safeLoad">7. <strong>全局替换 <code>yaml.safeLoad</code></strong></h4>
<ul>
<li>如果你的项目中还有其他地方使用了 <code>yaml.safeLoad</code>，可以全局替换为 <code>yaml.load</code>。</li>
<li>使用代码编辑器的全局搜索功能，查找并替换所有 <code>yaml.safeLoad</code> 为 <code>yaml.load</code>。</li>
</ul>
<hr>
<h4 id="8-提交-Issue-给-Hexo-Admin">8. <strong>提交 Issue 给 Hexo Admin</strong></h4>
<ul>
<li>如果以上方法无效，建议到 Hexo Admin 的 GitHub 仓库提交 Issue，说明你的问题和环境信息（如 Hexo 版本、Node.js 版本等）。</li>
<li>Hexo Admin 仓库地址：<a href="https://github.com/hexojs/hexo-admin">https://github.com/hexojs/hexo-admin</a></li>
</ul>
<hr>
<h4 id="9-临时解决方案">9. <strong>临时解决方案</strong></h4>
<ul>
<li>如果你急需解决问题，可以暂时锁定 <code>js-yaml</code> 的版本为 3.x，在 <code>package.json</code> 中添加以下内容：<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;resolutions&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;js-yaml&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3.14.1&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li>然后运行 <code>npm install</code> 重新安装依赖。</li>
</ul>
<hr>
<p>通过以上方法，你应该能够解决 <code>yaml.safeLoad</code> 被移除的问题。</p>
<h3 id="三-Hexo的next主题markdown列表无法正常渲染的问题">三. Hexo的next主题markdown列表无法正常渲染的问题</h3>
<h4 id="解决方案：">解决方案：</h4>
<ul>
<li>
<p>修改custom.css样式</p>
</li>
<li>
<p>打开 <code>themes/next/source/css/custom.css</code> 添加以下代码修复列表样式：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> /* 强制覆盖主题样式 */</span><br><span class="line">.post .post-body ul,</span><br><span class="line">.post .post-body ol &#123;</span><br><span class="line">  padding-left: 2em !important;</span><br><span class="line">  list-style-type: disc !important;  /* 无序列表样式 */</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* 有序列表样式 */</span><br><span class="line">.post .post-body ol &#123;</span><br><span class="line">  list-style-type: decimal !important;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>通过主题中的_config.yml自定义样式，加载custom.css文件</p>
<ul>
<li>
<p>打开 themes/next/_config.yml</p>
</li>
<li>
<p>找到 custom_file_path 部分，确保 style 指向正确路径：</p>
</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">	style: source/css/custom.css  # 取消注释</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>清理缓存重新生成：</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="四-Hexo整体迁移到新的电脑上，执行hexo-clean-hexo-g-hexo-d报错">四. Hexo整体迁移到新的电脑上，执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d报错</h3>
<h4 id="解决方案：-2">解决方案：</h4>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 错误信息：fatal: detected dubious ownership in repository at &#x27;D:/AIGC/my_blog/.deploy_git&#x27;</span><br><span class="line"># 因为 Git 检测到仓库的所有权与当前用户不匹配</span><br><span class="line"># 使用以下命令：添加安全目录就好了</span><br><span class="line">git config --global --add safe.directory D:/AIGC/my_blog/.deploy_git</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>自媒体</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG优化技术</title>
    <url>/2025/03/28/RAG%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h4 id="之前的课程我们已经学完了RAG的完整流程，这里先做个回顾：">之前的课程我们已经学完了RAG的完整流程，这里先做个回顾：</h4>
<ul>
<li>
<p>索引阶段</p>
<ul>
<li>
<p>文档解析技术</p>
</li>
<li>
<p>分块策略</p>
<ul>
<li><strong>固定大小分块、重叠分块、递归分块、文档特定分块、语义分块、混合分块</strong></li>
</ul>
</li>
<li>
<p>嵌入模型选择</p>
</li>
</ul>
</li>
<li>
<p>检索阶段</p>
<ul>
<li>
<p>向量数据库选择</p>
</li>
<li>
<p>混合检索</p>
<ul>
<li>
<p>关键词检索</p>
</li>
<li>
<p>向量检索</p>
</li>
</ul>
</li>
<li>
<p>重排序</p>
</li>
</ul>
</li>
<li>
<p>生成阶段</p>
<ul>
<li>
<p>大语言模型选择</p>
</li>
<li>
<p>提示词工程</p>
</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h4 id="这节课我们来补充工程化RAG全流程优化技术-进一步提高RAG系统的检索精度和效率以及回答的准确度-：">这节课我们来补充工程化RAG全流程优化技术(<mark>进一步提高RAG系统的检索精度和效率以及回答的准确度</mark>)：</h4>
<ul>
<li>
<p>索引阶段（提升检索的精度和效率）</p>
<ul>
<li>
<p>除了常规的分块策略，这里介绍几个其他的知识入库的技术</p>
<ul>
<li>
<p><strong>按树入库</strong></p>
<ul>
<li>我们可以将知识按树入库。我们可以这样想象，我们像整理一本书那样将知识入库，例如按章、节、小节、段这样把内容整理成一棵树，其中每章包括哪些节，每节包括哪些小节，每小节包括哪些段。</li>
</ul>
</li>
<li>
<p><strong>分层入库</strong></p>
<ul>
<li>我们可以将知识分成多层，然后再入库。例如我们可以建立一个两层系统，一层是文本摘要，一层是详细内容，然后两者都包含指向数据中相同位置的元数据。</li>
</ul>
</li>
<li>
<p><strong>信息压缩入库</strong></p>
<ul>
<li>有些业务场景下，我们不需要把所有信息都入库，只需要把关键的信息入库，这时候我们可以使用大模型将与业务场景无关或者次要的信息先剔除掉，然后再入库。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>检索阶段</p>
<ul>
<li>
<p>综合应用多种过滤技术</p>
<ul>
<li>我们可以综合应用多种过滤技术来改进检索结果的质量。例如我们可以综合应用元数据（例如用户权限，知识库权限）、相似度、关键词、内容过滤（删除不符合特定内容标准或基本关键词的结果）、多样性过滤（通过过滤掉近似重复的条目来确保结果的多样性）</li>
</ul>
</li>
<li>
<p>投票和加权检索</p>
<ul>
<li>我们可以在混合检索之后，再加上<strong>投票和加权机制</strong>来确定最终检索到的知识。</li>
</ul>
</li>
<li>
<p>分解成子查询再整合</p>
<ul>
<li>我们可以将复杂的查询分解为更简单的子查询去检索知识，然后再整合在一起。</li>
</ul>
</li>
<li>
<p>多轮检索</p>
<ul>
<li>我们可以检索出一轮之后用大模型分析结果并生成后续查询，这样循环迭代，直到生成最终的结果。不过这种方法只能用在对实时性要求不高的业务场景里，例如每日生成报告的场景。</li>
</ul>
</li>
<li>
<p>查询扩展</p>
<ul>
<li>
<p>查询扩展策略通过大模型从原始查询语句生成<strong>多个语义相关的查询，可以覆盖向量空间中的不同区域</strong>，从而提高检索的全面性和准确性。这些查询在嵌入后能够击中不同的语义区域，确保系统能够从更广泛的文档中检索到与用户需求相关的有用信息。</p>
</li>
<li>
<p>查询扩展指令模板：</p>
  <figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">你是一个AI语言模型助手。</span><br><span class="line">你的任务是生成五个不同版本的用户问题，以便从向量数据库中检索相关文档。</span><br><span class="line">通过从多个角度生成用户问题，你的目标是帮助用户克服基于距离的相似性搜索的一些局限性。</span><br><span class="line">请将这些替代问题用换行符分隔。原始问题：&#123;查询原文&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>预生成问题和回答（命中缓存，撞库回答）</p>
<ul>
<li>我们可以按照一些模式去预先生成问题，然后使用前面的一些高级但是比较耗时的方法，例如 AI 智能体检索和多轮检索的方式来生成回答。然后把这些问题和回答存进知识库，当用户提问时，我们根据用户提问检索对应的问题，并返回对应的回答。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>生成阶段</p>
<ul>
<li>
<p>提示压缩</p>
<ul>
<li>
<p>提示压缩旨在减少上下文中的噪声，并突出最相关的信息，从而提高检索精度和生成质量。在 RAG 系统中，检索到的文档通常包含大量无关的文本，这些无关内容可能会掩盖与查询高度相关的信息，导致生成结果的相关性下降。提示压缩通过精简上下文、过滤掉不相关的信息，确保系统只处理与查询最相关、最重要的内容。</p>
</li>
<li>
<p>提示压缩的指令模版</p>
  <figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">你是一个AI语言模型助手，负责对检索到的文档进行上下文压缩。</span><br><span class="line">你的目标是从文档中提取与用户查询高度相关的段落，并删除与查询无关或噪声较大的部分。</span><br><span class="line">你应确保保留所有能够直接回答用户查询的问题核心信息。</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">用户查询：&#123;用户的原始查询&#125;</span><br><span class="line">检索到的文档：&#123;检索到的文档内容&#125;</span><br><span class="line"></span><br><span class="line">输出要求：</span><br><span class="line">提取与用户查询最相关的段落和信息。</span><br><span class="line">删除所有与查询无关的内容，包括噪声、背景信息或扩展讨论。</span><br><span class="line">压缩后的内容应简洁清晰，直指用户的核心问题。</span><br><span class="line"></span><br><span class="line">输出格式：</span><br><span class="line">&#123;压缩段落1&#125;</span><br><span class="line">&#123;压缩段落2&#125;</span><br><span class="line">&#123;压缩段落3&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>通过提示压缩，系统能够准确提取出与查询高度相关的核心信息，去除冗余内容，并返回简洁的压缩结果。组合成为新的指令，输入大模型获得回复，提高 RAG 系统答案准确度。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>优化技术</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>优化</tag>
        <tag>检索精度</tag>
        <tag>检索效率</tag>
      </tags>
  </entry>
  <entry>
    <title>LangChain链的类型及示例</title>
    <url>/2025/03/25/LangChain%E9%93%BE%E7%9A%84%E7%B1%BB%E5%9E%8B%E5%8F%8A%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<h1>LangChain 中的链（Chains）类型及示例</h1>
<p>LangChain 提供了多种类型的链（Chains），用于将多个组件组合起来完成复杂任务。以下是主要的链类型及示例：</p>
<h2 id="1-LLMChain（基础链）">1. LLMChain（基础链）</h2>
<p>最基本的链类型，将提示模板、LLM和输出解析器组合在一起。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;product&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;为&#123;product&#125;写一个创意广告文案:&quot;</span></span><br><span class="line">)</span><br><span class="line">llm = OpenAI(temperature=<span class="number">0.9</span>)</span><br><span class="line">chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(chain.run(<span class="string">&quot;环保水杯&quot;</span>))</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h2 id="2-SequentialChain（顺序链）">2. SequentialChain（顺序链）</h2>
<p>按顺序执行多个链，前一个链的输出作为下一个链的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> SequentialChain, LLMChain</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个链：生成广告文案</span></span><br><span class="line">prompt1 = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;product&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;为&#123;product&#125;写一个创意广告文案:&quot;</span></span><br><span class="line">)</span><br><span class="line">chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=<span class="string">&quot;ad_copy&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个链：评估广告效果</span></span><br><span class="line">prompt2 = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;ad_copy&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;评估以下广告的效果:\n&#123;ad_copy&#125;\n\n评估结果:&quot;</span></span><br><span class="line">)</span><br><span class="line">chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=<span class="string">&quot;review&quot;</span>)</span><br><span class="line"></span><br><span class="line">overall_chain = SequentialChain(</span><br><span class="line">    chains=[chain1, chain2],</span><br><span class="line">    input_variables=[<span class="string">&quot;product&quot;</span>],</span><br><span class="line">    output_variables=[<span class="string">&quot;ad_copy&quot;</span>, <span class="string">&quot;review&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = overall_chain(&#123;<span class="string">&quot;product&quot;</span>: <span class="string">&quot;智能手表&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<h2 id="3-TransformChain（转换链）">3. TransformChain（转换链）</h2>
<p>对输入进行转换而不调用LLM。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> TransformChain</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_func</span>(<span class="params">inputs: <span class="built_in">dict</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    text = inputs[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;output_text&quot;</span>: text.upper()&#125;</span><br><span class="line"></span><br><span class="line">transform_chain = TransformChain(</span><br><span class="line">    input_variables=[<span class="string">&quot;text&quot;</span>], </span><br><span class="line">    output_variables=[<span class="string">&quot;output_text&quot;</span>],</span><br><span class="line">    transform=transform_func</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">transform_chain.run(&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;hello world&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="4-RouterChain（路由链）">4. RouterChain（路由链）</h2>
<p>根据输入决定使用哪个子链。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.router <span class="keyword">import</span> MultiPromptChain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.router.llm_router <span class="keyword">import</span> LLMRouterChain, RouterOutputParser</span><br><span class="line"></span><br><span class="line">physics_template = <span class="string">&quot;&quot;&quot;你是一位物理专家。回答物理问题:</span></span><br><span class="line"><span class="string">问题: &#123;input&#125;&quot;&quot;&quot;</span></span><br><span class="line">math_template = <span class="string">&quot;&quot;&quot;你是一位数学专家。回答数学问题:</span></span><br><span class="line"><span class="string">问题: &#123;input&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">prompt_infos = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;physics&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;适合回答物理问题&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt_template&quot;</span>: physics_template</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;math&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;适合回答数学问题&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt_template&quot;</span>: math_template</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">router_chain = MultiPromptChain.from_prompts(</span><br><span class="line">    llm,</span><br><span class="line">    prompt_infos,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(router_chain.run(<span class="string">&quot;什么是相对论?&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(router_chain.run(<span class="string">&quot;2+2等于几?&quot;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="5-APIChain（API链）">5. APIChain（API链）</h2>
<p>与外部API交互的链。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> APIChain</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">api_docs = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">API文档:</span></span><br><span class="line"><span class="string">BASE URL: https://api.example.com</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">端点:</span></span><br><span class="line"><span class="string">GET /weather?location=&#123;location&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">- location: 城市名称</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">响应:</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;temp&quot;: 温度,</span></span><br><span class="line"><span class="string">    &quot;condition&quot;: 天气状况</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">chain = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=<span class="literal">True</span>)</span><br><span class="line">chain.run(<span class="string">&quot;上海现在的天气如何?&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="6-SQLDatabaseChain（数据库链）">6. SQLDatabaseChain（数据库链）</h2>
<p>与SQL数据库交互的链。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> SQLDatabase, SQLDatabaseChain</span><br><span class="line"></span><br><span class="line">db = SQLDatabase.from_uri(<span class="string">&quot;sqlite:///chinook.db&quot;</span>)</span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=<span class="literal">True</span>)</span><br><span class="line">db_chain.run(<span class="string">&quot;有多少位顾客?&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="7-RetrievalQAChain（检索问答链）">7. RetrievalQAChain（检索问答链）</h2>
<p>结合检索器和LLM的问答链。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line">loader = TextLoader(<span class="string">&quot;state_of_the_union.txt&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">docsearch = FAISS.from_documents(texts, embeddings)</span><br><span class="line"></span><br><span class="line">qa = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=OpenAI(),</span><br><span class="line">    chain_type=<span class="string">&quot;stuff&quot;</span>,</span><br><span class="line">    retriever=docsearch.as_retriever()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;总统提到了什么重要政策?&quot;</span></span><br><span class="line">qa.run(query)</span><br></pre></td></tr></table></figure>
<h2 id="8-ConversationChain（对话链）">8. ConversationChain（对话链）</h2>
<p>维护对话上下文的链。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> ConversationChain</span><br><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationBufferMemory</span><br><span class="line"></span><br><span class="line">conversation = ConversationChain(</span><br><span class="line">    llm=OpenAI(),</span><br><span class="line">    memory=ConversationBufferMemory(),</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">conversation.predict(<span class="built_in">input</span>=<span class="string">&quot;你好!&quot;</span>)</span><br><span class="line">conversation.predict(<span class="built_in">input</span>=<span class="string">&quot;我很好，谢谢!你呢?&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="9-MapReduceChain（映射归约链）">9. MapReduceChain（映射归约链）</h2>
<p>处理长文档的链，先映射处理各部分，再归约总结。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> MapReduceChain</span><br><span class="line"></span><br><span class="line">map_prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;page&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;总结以下文本:\n&#123;page&#125;&quot;</span></span><br><span class="line">)</span><br><span class="line">reduce_prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;summaries&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;结合以下总结写出最终总结:\n&#123;summaries&#125;&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = MapReduceChain.from_params(</span><br><span class="line">    llm=llm,</span><br><span class="line">    map_prompt=map_prompt,</span><br><span class="line">    reduce_prompt=reduce_prompt</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;long_document.txt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    long_text = f.read()</span><br><span class="line">    </span><br><span class="line">result = chain.run(&#123;<span class="string">&quot;input_text&quot;</span>: long_text&#125;)</span><br></pre></td></tr></table></figure>
<p>这些链可以单独使用，也可以组合起来构建更复杂的应用。LangChain还允许创建自定义链以满足特定需求。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>langchain</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>langchain</tag>
        <tag>chains</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG向量数据库原理与常用向量库</title>
    <url>/2025/03/11/RAG%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%B8%B8%E7%94%A8%E5%90%91%E9%87%8F%E5%BA%93/</url>
    <content><![CDATA[<h3 id="在前面的课程中，我们已经学习了-RAG-检索流程中如何将文档数据解析、分块并转换为嵌入向量的操作。本节课将进一步掌握如何存储这些向量及其文档元数据，并高效地进行相似度检索。">在前面的课程中，我们已经学习了 RAG 检索流程中如何将文档数据解析、分块并转换为嵌入向量的操作。本节课将进一步掌握如何存储这些向量及其文档元数据，并高效地进行相似度检索。</h3>
<h3 id="在人工智能（AI）主导的时代，文字、图像、语音、视频等多模态数据的复杂性显著增加。由于这些数据具有非结构化和多维特征，向量表示能够有效表示语义和捕捉其潜在的语义关系，促使向量数据库成为存储、检索和分析高维数据向量的关键工具。">在人工智能（AI）主导的时代，文字、图像、语音、视频等多模态数据的复杂性显著增加。由于这些数据具有非结构化和多维特征，向量表示能够有效表示语义和捕捉其潜在的语义关系，促使向量数据库成为存储、检索和分析高维数据向量的关键工具。</h3>
<span id="more"></span>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311110709348.png" alt="image-20250311110709348"></p>
<h4 id="下图展示了向量数据库的分类，依据是否开源与是否为专用向量数据库，将其分为四类。">下图展示了向量数据库的分类，依据是否开源与是否为专用向量数据库，将其分为四类。</h4>
<ol>
<li>第一类是开源的专用向量数据库，如 Chroma、Vespa、LanceDB、Marqo、Qdrant 和 Milvus，这些数据库专门设计用于处理向量数据。</li>
<li>第二类是支持向量搜索的开源数据库，如 OpenSearch、PostgreSQL、ClickHouse 和 Cassandra，它们是常规数据库，但支持向量搜索功能。</li>
<li>第三类是商用的专用向量数据库，如 Weaviate 和 Pinecone，它们专门用于处理向量数据，但属于商业产品或通过商业许可获得源码。</li>
<li>第四类是支持向量搜索的商用数据库，如 Elasticsearch、Redis、Rockset 和 SingleStore，这些常规数据库支持向量搜索功能，同时属于商业产品或可通过商业许可获得源码。</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311110911756.png" alt="image-20250311110911756"></p>
<h3 id="为什么需要向量数据库？">为什么需要向量数据库？</h3>
<p><strong>传统数据库通常分为关系型（SQL）数据库和非关系型（NoSQL）数据库</strong>，其中存储复杂、非结构化或半结构化信息的需求主要依赖于非关系型数据库的能力。<strong>图中展示了三种非关系型数据库类型与向量数据库</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311111250193.png" alt="image-20250311111250193"></p>
<ol>
<li>键值数据库（Key-Value）：通常用于简单的数据存储，通过键来快速访问数据。</li>
<li>文档数据库（Document）：用于存储文档结构的数据，如 JSON 格式。</li>
<li>图数据库（Graph）：用于表示和存储复杂的关系数据，常用于社交网络、推荐等场景。</li>
<li>向量数据库（Vector）：用于存储和检索基于向量表示的数据，用于 AI 模型的高维度和复杂的嵌入向量。</li>
</ol>
<h4 id="那在什么场景下该选择什么样的数据库呢？">那在什么场景下该选择什么样的数据库呢？</h4>
<ul>
<li>
<p>举个例子，如果你想要找到一本特定的书，只需通过书名来精准定位信息，键值数据库是最理想的选择。而如果你需要查询一本书的详细章节内容、作者简介等复杂的结构化信息，文档数据库则更为适用。</p>
</li>
<li>
<p>如果你的目标是了解书籍之间的推荐关系，或者探索作者之间的合作网络，图数据库可以高效存储和查询这些复杂的关系数据。</p>
</li>
<li>
<p>最后，如果你希望找到与某本书内容相似的书籍，比如基于主题、风格等特征进行相似性搜索，向量数据库则能够通过计算书籍内容语义在向量空间中的距离，为你提供语义最相关的数据信息。</p>
</li>
<li>
<p>向量数据库的核心在于其能够基于向量之间的相似性，快速、精确地定位和检索数据。这类数据库不仅为向量嵌入提供了优化的存储和查询功能，同时也继承了传统数据库的诸多优势，如性能、可扩展性和灵活性，满足了充分利用大规模数据的需求。相比之下，传统的基于标量的数据库由于无法应对数据复杂性和规模化处理的挑战，难以有效提取洞察并实现实时分析。</p>
</li>
</ul>
<h4 id="向量数据库的主要优势体现在以下几个方面：">向量数据库的主要优势体现在以下几个方面：</h4>
<ol>
<li>数据管理：向量数据库提供了易于使用的数据存储功能，如插入、删除和更新操作。与独立的向量索引工具（如 Faiss）相比，这使得向量数据的管理和维护更加简便，因为 Faiss 需要额外的工作才能与存储解决方案集成。</li>
<li>元数据存储和筛选：向量数据库能够<strong>存储与每个向量条目关联的元数据</strong>，用户可以基于这些元数据进行更细粒度的查询，从而提升查询的精确度和灵活性。</li>
<li>可扩展性：向量数据库设计旨在应对不断增长的数据量和用户需求，支持分布式和并行处理，并通过无服务器架构优化大规模场景下的成本。</li>
<li>实时更新：向量数据库通常支持实时数据更新，允许动态修改数据以确保检索结果的时效性和准确性。</li>
<li>备份与恢复：向量数据库具备完善的备份机制，能够处理数据库中所有数据的例行备份操作，确保数据的安全性与持久性。</li>
<li>生态系统集成：向量数据库能够与数据处理生态系统中的其他组件（如 ETL 管道中的 Spark、分析工具如 Tableau 和 Segment、可视化平台如 Grafana）轻松集成，从而简化数据管理工作流程。此外，它还能够无缝集成 AI 相关工具，如 <strong>LangChain、LlamaIndex</strong> 和 Cohere，进一步增强其应用潜力。</li>
<li>数据安全与访问控制：向量数据库通常提供内置的数据安全功能和访问控制机制，以保护敏感信息。通过命名空间实现的多租户管理，允许用户对索引进行完全分区，甚至可以在各自的索引中创建完全隔离的分区，确保数据的安全性和访问的灵活性。</li>
</ol>
<p>由于其上述特性，向量数据库可以广泛应用于 <strong>LLM RAG 系统</strong>、推荐系统、异常检测、计算机视觉、自然语言处理等多种 AI 产品生产场景中。</p>
<p>综上所述，向量数据库是一类专门为生产场景下的向量嵌入管理而构建的数据库。与传统的基于标量的数据库及独立的向量索引相比，向量数据库在性能、可扩展性、安全性和生态系统集成等方面展现了显著的优势，为现代数据管理提供了强有力的支持。</p>
<h4 id="向量数据库是如何工作的？">向量数据库是如何工作的？</h4>
<ul>
<li>
<p>向量数据库是一种专门用于存储和检索多维向量的数据库类型，与传统的基于行列结构的数据库不同，它主要处理高维空间中的数据点。<strong>传统数据库通常处理字符串、数字等标量数据，并通过<mark>精确匹配</mark>来查询数据</strong>。然而，<strong>向量数据库的操作逻辑则是基于<mark>相似性搜索</mark></strong>，即在查询时，应用特定的相似性度量（如余弦相似度、欧几里得距离等）来查找与查询向量最相似的向量。</p>
</li>
<li>
<p>向量数据库的核心在于其高效的索引和搜索机制。为了优化查询性能，它采用了如哈希、量化和基于图形的多种算法。这些算法通过构建如<strong>层次化可导航小世界（HNSW）图、产品量化（PQ）和位置敏感哈希（LSH）等索引结构</strong>，显著提升了查询速度。这种搜索过程并非追求绝对精确，而是<strong>通过近似最近邻（ANN）算法在速度与准确性之间进行权衡</strong>，从而实现快速响应。</p>
</li>
<li>
<p>向量数据库的索引结构可以理解为一种预处理步骤，类似于为图书馆中的书籍编制索引，方便快速找到所需内容。HNSW 图通过在多层结构中将相似向量连接在一起，快速缩小搜索范围。PQ 则通过压缩高维向量，减少内存占用并加速检索，而 LSH 则通过哈希函数将相似向量聚集在一起，便于快速定位。</p>
</li>
<li>
<p>向量数据库的搜索机制不是追求精确匹配，而是通过近似最近邻（ANN）算法在速度与准确性之间找到最佳平衡。ANN 算法通过允许一定程度的误差，在显著提高搜索速度的同时，依然能够找到与查询相似度较高的向量。这种策略对于需要实时、高精度响应的应用场景尤为重要。</p>
</li>
</ul>
<h5 id="向量数据库的工作流程涵盖了从向量存储、向量索引到最终检索的多环节操作，确保在复杂的数据环境中实现高效的存储、索引和相似性搜索。具体流程如下：">向量数据库的工作流程涵盖了从向量存储、向量索引到最终检索的多环节操作，确保在复杂的数据环境中实现高效的存储、索引和相似性搜索。具体流程如下：</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311112811861.png" alt="image-20250311112811861"></p>
<ol>
<li><strong>数据处理与向量化</strong>：原始数据首先被处理并转化为向量嵌入。这一步通过嵌入模型实现，模型利用深度学习算法提取数据的语义特征，生成适合后续处理的高维向量表示。</li>
<li><strong>向量存储</strong>：转化后的向量嵌入存储在数据库中。这一环节确保数据在高效检索的同时，能够以优化的方式管理和维护存储资源，以适应不同规模和复杂度的应用需求。</li>
<li><strong>向量索引</strong>：存储的向量嵌入需要经过索引处理，以便在后续查询中快速定位相关数据。索引过程通过构建特定的结构，使得数据库能够在大规模数据集上实现高效的查询响应。</li>
<li><strong>向量搜索</strong>：在接收到查询后，数据库通过已建立的索引结构执行相似性搜索，找出与查询向量最为接近的数据点。这一阶段的重点在于平衡搜索的速度与准确性，确保在大数据环境下提供快速且相关的查询结果。常见的向量搜索方法包括余弦相似度、欧几里得距离和曼哈顿距离。其中，<strong>余弦相似度主要用于文本处理和信息检索</strong>，关注向量之间的角度，以捕捉语义相似性；欧几里得距离则测量向量之间的实际距离，适用于密集特征集的聚类或分类；而曼哈顿距离则通过计算笛卡尔坐标中的绝对差值之和，适用于稀疏数据的处理。</li>
<li><strong>数据检索</strong>：最后，数据库从匹配的向量中检索出对应的原始数据，并根据特定的需求进行必要的后处理。这一步骤确保最终结果能够准确反映用户的查询意图，并提供有意义的输出。</li>
</ol>
<p>在 RAG 系统中，向量数据库起着重要的作用。其主要功能在于索引过程中，<strong>建立高效的向量索引结构</strong>，以便快速定位与查询相关的向量数据。在查询阶段，系统将输入的提示转化为向量表示形式，并从数据库中<strong>检索出与之最相关的向量及其对应的分块数据</strong>。通过这种索引和检索机制，检索到的向量为生成模型提供了必要的上下文信息，使模型能够依据当前的语义上下文生成更加精准和相关的响应。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311114141538.png" alt="image-20250311114141538"></p>
<h3 id="常用的向量数据库">常用的向量数据库</h3>
<p>下面列出十个目前主流的向量数据库，展示其数据库链接、介绍、优点与缺点。根据开发者具体的使用场景和技术需求，选择最适合的向量数据库解决方案是关键。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250311114333810.png" alt="image-20250311114333810"></p>
<p>根据上面所示特点：</p>
<ul>
<li>对于需要快速开发和轻量化部署的项目，Chroma、Qdrant 是不错的选择。</li>
<li>而对于追求高性能和可扩展性的企业级应用，可以考虑 Milvus/Zilliz。</li>
<li>FAISS 是适合对性能有极致要求、不要求持久化和数据管理的场景。</li>
<li>Weaviate、LanceDB 在处理多模态数据方面表现突出，适用于需要管理多种数据类型（如图像、文本、音频等）的 AI 应用。</li>
<li>如果需要无缝集成现有数据库并进行向量搜索，PGVector、Elasticsearch、Redis 是理想的方案。</li>
<li>而不希望管理基础设施的用户则可以选择 Pinecone 这样的全托管服务。</li>
</ul>
<h3 id="向量数据库实战">向量数据库实战</h3>
<ul>
<li>
<p>在实战中，我们使用 Chroma 作为 RAG 项目的向量库，以替代原先无法持久化的 Faiss 库。</p>
</li>
<li>
<p>Chroma 是一种简单且易于持久化的向量数据库，它以轻量级、开箱即用的特性著称。Chroma 支持内存中操作和磁盘持久化，能够高效地管理和查询向量数据，非常适合快速集成和开发。其设计简洁且不需要复杂的配置，使开发者能够专注于核心功能的实现而无需担心底层存储的复杂性。</p>
</li>
</ul>
<ol>
<li>引入 Chroma 向量数据库 chromadb，引入 uuid 模块用于为每个文本块生成唯一的 ID。</li>
<li>在 main 方法中，创建了 Chroma 本地存储实例 client 和存储集合 collection，实例数据库存储在项目根目录 rag_learning/chroma_db 下，数据存储在 documents 集合中。</li>
<li>在 indexing_process 方法中，将文档切块后的文本块的 ID、嵌入向量和原始文本块内容存储到 ChromaDB 的 documents 集合中。</li>
<li>在 retrieval_process 方法中，使用 Chroma 向量数据库检索与查询（query）最相似的 top_k 个文本块。</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>索引/检索</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>向量数据库</tag>
        <tag>检索</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG检索流程（一）：混合检索</title>
    <url>/2025/03/12/RAG%E6%A3%80%E7%B4%A2%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2/</url>
    <content><![CDATA[<h3 id="为什么需要混合检索？">为什么需要混合检索？</h3>
<p>我们本节课正式开始讲解 <strong>RAG 检索流程</strong>。当前主流的 RAG 检索方式主要采用<strong>向量检索（Vector Search）</strong>，通过语义相似度来匹配文本切块。这种方法在我们之前的课程中已经深入探讨过了。然而，向量检索并非万能，它在某些场景下无法替代<strong>传统关键词检索</strong>的优势。</p>
<p>例如，当你需要精准搜索某个订单 ID、品牌名称或地址，或者搜索特定人物或物品的名字（如伊隆·马斯克、iPhone 15）时，<strong>向量检索的<mark>准确性</mark>往往不如关键词检索</strong>。此外，当用户输入的<strong>问题非常简短</strong>，仅包含几个单词时，比如搜索缩写词或短语（如 RAG、LLM），<strong>语义匹配的效果也可能不尽理想</strong>。</p>
<p>这些正是传统关键词检索的优势所在。关键词检索（Keyword Search）在几个场景中表现尤为出色：<strong>精确匹配</strong>，如产品名称、姓名、产品编号；少量字符的匹配，用户习惯于输入几个关键词，而少量字符进行向量检索时效果可能较差；以及<strong>低频词汇的匹配</strong>，低频词汇往往承载了关键意义，如在“你想跟我去喝咖啡吗？”这句话中，“喝”“咖啡”比“你”“吗”更具重要性。</p>
<span id="more"></span>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250312122858648.png" alt="image-20250312122858648"></p>
<p>在上述案例中，虽然依靠<strong>关键词检索可以精确找到与“订单 12345”匹配的特定信息</strong>，<strong>但它无法提供与订单相关的更广泛上下文</strong>。另一方面，<strong>语义匹配</strong>虽然能够识别“订单”和“配送”等相关概念，但在处理具体的订单 ID 时，往往容易出错。</p>
<p><strong><mark>混合检索（Hybrid Search）通过结合关键词检索和语义匹配的优势</mark></strong>，可以首先利用关键词检索精确定位到“订单 12345”的信息，然后通过语义匹配扩展与该订单相关的其他上下文或客户操作的信息，例如“12 开头的订单、包装破损严重”等。这样不仅能够获取精确的订单详情，还能获得与之相关的额外有用信息。</p>
<p>在 RAG 检索场景中，<strong>首要目标是确保最相关的结果能够出现在候选列表中</strong>。向量检索和关键词检索各有其独特优势，混合检索通过结合这多种检索技术，弥补了各自的不足，提供了一种更加全面的搜索方案。</p>
<h3 id="混合检索（多路召回）">混合检索（多路召回）</h3>
<p>混合检索是指在检索过程中同时采用多种检索方式，并将各类检索结果进行融合，从而得到最终的检索结果。<strong>混合检索的优势在于能够充分利用不同检索方式的优点，弥补各自的不足，从而提升检索的准确性和效率</strong>。下图展示了混合检索的流程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250312133821822.png" alt="image-20250312133821822"></p>
<p>混合检索实际上并没有严格限定必须包含哪几种检索方式。这里我们以<strong>向量检索</strong>和<strong>关键词检索</strong>的组合为例，但实际上可以包含多种检索方式的组合。如果我们将其他搜索算法结合在一起，也同样可以称为“混合检索”。例如，可以将**知识图谱（graphRAG）**技术用于检索实体关系，并与向量检索技术相结合。</p>
<p>更多的 RAG 检索方式还包括多重提问检索、上下文压缩检索、集成检索、多向量检索、自查询检索等，每种检索方式说明如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250312134208626.png" alt="image-20250312134208626"></p>
<p>上述方法均包含在 LangChain 的检索器模块 <strong>langchain.retrievers</strong> 中，具体详情可以查看 <a href="https://python.langchain.com/api_reference/langchain/retrievers.html">LangChain 检索器站点</a>。</p>
<p>混合检索后，我们需要对多个检索方式的检索结果进行综合排名。下节课会详细讲解重排序技术，这里我们简单了解一种更简单的方法来排序，称为<strong>递归折减融合（Reciprocal Rank Fusion, RRF）排序</strong>。RRF 是一种把来自不同检索方法的排名结果结合起来的技巧。它的基本思想是，如果一个文档在不同的检索结果中都排得比较靠前，那么它在综合排名中就应该得到更高的位置。</p>
<p>相比于复杂的重排序技术，RRF 的操作更加简单，不需要对每种检索结果进行复杂的调整或计算。它通过直接考虑文档在不同方法中的排名，快速生成一个合理的综合排名。这种方法非常适合那些需要快速融合多个检索结果的场景，短时间内得到一个有参考价值的排序。</p>
<p>选择何种检索技术，取决于开发者需要解决什么样的问题，系统的性能要求、数据的复杂性以及用户的搜索习惯等。针对具体需求选择合适的检索技术，能够最大化地提升 RAG 系统的效率和准确性。</p>
<h3 id="混合检索技术实战">混合检索技术实战</h3>
<p>在实战中，我们使用 <strong>rank_bm25</strong> 作为 RAG 项目的关键词搜索技术。BM25 是一种强大的关键词搜索算法，通过分析词频（TF）和逆向文档频率（IDF）来评估文档与查询的相关性。具体来说，BM25 检查查询词在文档中的出现频率，以及该词在所有文档中出现的稀有程度。<strong>如果一个词在特定文档中频繁出现，但在其他文档中较少见，那么 BM25 会将该文档评为高度相关。</strong></p>
<p>此外，BM25 还通过调整文档长度的影响，防止因文档长度不同而导致的词频偏差。正是这种结合了词频和文档长度平衡的机制，使得 BM25 在关键词搜索中能够提供精准的检索结果，在 RAG 项目中尤为有效。</p>
<p>需要安装的依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install -U pip jieba rank_bm25 chromadb langchain langchain_community sentence-transformers unstructured pdfplumber python-docx python-pptx markdown openpyxl pandas -i https://pypi.tuna.tsinghua.edu.cn/simple </span><br></pre></td></tr></table></figure>
<p>代码内容：</p>
<ol>
<li>引入了 rank_bm25 库中的 <strong>BM25Okapi</strong> 类，用于实现 BM25 算法的检索功能。</li>
<li>引入了 <strong>jieba</strong> 库，用于对中文文本进行分词处理，这对于 BM25 算法处理中文文本起关键作用。</li>
<li>在 <strong>retrieval_process</strong> 方法中，从 Chroma 的 collection 中提取所有存储的文档内容，并使用 jieba 对这些文档进行中文分词，将分词结果存储为 <strong>tokenized_corpus</strong>，为后续的 BM25 检索做准备。</li>
<li>利用分词后的文档集合实例化 BM25Okapi 对象，并对查询语句进行分词处理。</li>
<li>计算查询语句与每个文档之间的 BM25 相关性得分 (bm25_scores)，然后选择得分最高的前 top_k 个文档，并提取这些文档的内容。</li>
<li>返回合并后的全部检索结果，包含向量检索和 BM25 检索的结果。（下节内容会引入重排序，这里只做简单顺序合并）</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>检索</category>
        <category>混合检索</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>检索</tag>
        <tag>混合检索</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG检索流程（二）：重排序</title>
    <url>/2025/03/19/RAG%E6%A3%80%E7%B4%A2%E6%B5%81%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%87%8D%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="重排序（Reranking）的目的是将混合检索的结果进行整合，并将与用户问题语义最契合的结果排在前列。">重排序（Reranking）的目的是将混合检索的结果进行整合，并将与用户问题语义最契合的结果排在前列。</h4>
<p>下图中仅仅混合检索，由于缺乏有效的排序，我们期望的结果位于第一和第四位，尽管依然可以被检索到，但理想情况下，如果检索方式更为精确，该结果应该被优先排序在前两位。</p>
<span id="more"></span>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250319155715601.png" alt="image-20250319155715601"></p>
<p>在这个案例中，我们通过重排序技术成功找到了与问题语义最契合的结果。系统评分显示，“订单 12345 于 2023 年 8 月 15 日在上海，客户不满意。”与“该 12 开头的订单客户不满意的地方在于包装破损严重。”这两个文档块的相关性分别为 0.9 和 0.8，排序为第一和第二位。</p>
<p>重排序技术在检索系统中扮演着至关重要的角色。即使检索算法已经能够捕捉到所有相关的结果，重排序过程依然不可或缺。它确保最符合用户意图和查询语义的结果优先展示，从而提升用户的搜索体验和结果的准确性。通过重排序，检索系统不仅能找到相关信息，还能智能地将最重要的信息呈现在用户面前。</p>
<h4 id="为什么要使用重排序技术？">为什么要使用重排序技术？</h4>
<p>在 RAG 检索流程中，重排序技术（Reranking）通过对初始检索结果进行重新排序，改善检索结果的相关性，为生成模型提供更优质的上下文，从而提升整体 RAG 系统的效果。</p>
<p>尽管向量检索技术能够为每个文档块生成初步的相关性分数，但引入重排序模型仍然至关重要。<strong>向量检索</strong>主要依赖于<strong>全局语义相似性</strong>，通过将查询和文档映射到高维语义空间中进行匹配。然而，这种方法往往忽略了查询与文档具体内容之间的细粒度交互。</p>
<p><strong>重排序模型大多是基于双塔或交叉编码架构的模型</strong>，在此基础上进一步计算更精确的相关性分数，能够<strong>捕捉查询词与文档块之间更细致的相关性</strong>，从而在细节层面上提高检索精度。因此，尽管向量检索提供了有效的初步筛选，重排序模型则通过更深入的分析和排序，确保最终结果在语义和内容层面上更紧密地契合查询意图，实现了检索质量的提升。</p>
<p>2023 年，Microsoft Azure AI 发布了《Azure 认知搜索：通过混合检索和排序能力超越向量搜索》一文。这篇文章对在 LLM RAG 应用中引入混合检索和重排序技术进行了全面的实验数据评估，并量化了这些技术组合在提升文档召回率和准确性方面的显著效果。实验结果表明，在多个数据集和多种检索任务中，混合检索与重排序的组合均取得了最佳表现。</p>
<div style="display: flex;">
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250321150711668.png" alt="Image 1" style="width: 50%;">
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250321150950573.png" alt="Image 2" style="width: 50%;">
</div>
<p>以下是使用重排序技术的几个优势：</p>
<ol>
<li>
<p>优化检索结果</p>
<p>在 RAG 系统中，初始的检索结果通常来自于向量搜索或基于关键词的检索方法。然而，这些初始检索结果可能包含大量的冗余信息或与查询不完全相关的文档。通过重排序技术，我们可以对这些初步检索到的文档进行进一步的筛选和排序，将最相关、最重要的文档置于前列。</p>
</li>
<li>
<p>增强上下文相关性</p>
<p>RAG 系统依赖于检索到的文档作为生成模型的上下文。因此，上下文的质量直接影响生成的结果。重排序技术通过重新评估文档与查询的相关性，确保生成模型优先使用那些与查询最相关的文档，从而提高了生成内容的准确性和连贯性。</p>
</li>
<li>
<p>应对复杂查询</p>
<p>对于复杂的查询，初始检索可能会返回一些表面上相关但实际上不太匹配的文档。重排序技术可以根据查询的复杂性和具体需求，对这些结果进行更细致的分析和排序，优先展示那些能够提供深入见解或关键信息的文档。</p>
</li>
</ol>
<h4 id="重排序模型-Reranking-Model">重排序模型 Reranking Model</h4>
<p>RAG 流程有两个概念，粗排和精排。粗排检索效率较快，但是召回的内容并不一定强相关。而精排效率较低，因此适合在粗排的基础上进行进一步优化。精排的代表就是重排序（Reranking）。</p>
<p>**重排序模型（Reranking Model）**查询与每个文档块计算对应的相关性分数，并根据这些分数对文档进行重新排序，确保文档按照从最相关到最不相关的顺序排列，并返回前 top-k 个结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250321143305625.png" alt="image-20250321143305625"></p>
<p><strong>与嵌入模型不同，重排序模型将用户的查询（Query）和文档块作为输入，直接输出相似度评分</strong>，而非生成嵌入向量。目前，市面上可用的重排序模型并不多，商用的有 Cohere，<strong>开源的有 BGE</strong>、Sentence、Mixedbread、T5-Reranker 等，<strong>甚至可以使用指令</strong>（Prompt）让大模型（GPT、Claude、通义千问、文心一言等）进行重排，大模型指令参考如下：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">以下是与查询 &#123;问题&#125; 相关的文档块：</span><br><span class="line">[1] &#123;文档块1&#125;</span><br><span class="line">[2] &#123;文档块2&#125;</span><br><span class="line">（更多文档块）</span><br><span class="line">请根据这些文档块与查询的相关性进行排序，以 “1,2,3,4”（文档块数字及逗号隔开的形式），输出排序结果。</span><br></pre></td></tr></table></figure>
<p>在生产环境中使用重排序模型会面临资源和效率问题，包括计算资源消耗高、推理速度慢以及模型参数量大等问题。这些问题主要源于重排序模型在对候选项进行精细排序时，因其较大参数量而导致的高计算需求和复杂耗时的推理过程，从而对 RAG 系统的响应时间和整体效率产生负面影响。因此，在实际应用中，需要根据实际资源情况，在精度与效率之间进行平衡。</p>
<h4 id="重排序技术实战">重排序技术实战</h4>
<p>在实战中，我们使用来自北京人工智能研究院 BGE 的 <strong>bge-reranker-v2-m3</strong> 作为 RAG 项目的重排序模型，这是一种轻量级的开源和多语言的重排序模型。更多模型相关信息参考，可访问<a href="https://huggingface.co/BAAI/bge-reranker-v2-m3">https://huggingface.co/BAAI/bge-reranker-v2-m3</a></p>
<p>这部分的代码文件为 rag_lesson_6。可到我的 Github 托管项目(<a href="https://github.com/hfhfn/rag_learning">https://github.com/hfhfn/rag_learning</a>)下载，或使用 <code>git clone https://github.com/hfhfn/rag_learning.git</code> ，拉取最新代码.</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>检索</category>
        <category>重排序</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>检索</tag>
        <tag>重排序</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG生成（一）：大模型的选择</title>
    <url>/2025/03/22/RAG%E7%94%9F%E6%88%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%89%E6%8B%A9/</url>
    <content><![CDATA[<h4 id="RAG的生成流程">RAG的生成流程</h4>
<p>生成流程中，首先需要<strong>组合指令</strong>，指令将<strong>携带查询问题及检索到的相关信息</strong>输入到大模型中，由<strong>大模型理解并生成最终的回复</strong>，从而完成整个应用过程。</p>
<p>在这个过程中，有两个环节直接影响RAG系统的生成效果：</p>
<p><strong>1. 大模型的选择</strong>：大模型相当于RAG系统的大脑，决定着RAG系统的响应质量。</p>
<p><strong>2. 提示词工程</strong>：通过有效的指令的设计和组合，可以帮助大模型更好的理解内容和生成更加精确和相关的回答。</p>
<span id="more"></span>
<h4 id="大模型的发展">大模型的发展</h4>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250322185744991.png" alt="image-20250322185744991"></p>
<p><strong>SuperCLUE</strong> 组织发布的<strong>中文大模型全景图</strong>，展示了 2025 年值得关注的中文大模型，从<strong>文本、多模态、行业</strong>三个层面进行了详细分类，各领域的大模型应用层出不穷。<strong>RAG 中目前更关注通用大模型</strong>，比如<strong>闭源的文心一言、通义千问、腾讯混元、字节豆包</strong>、Kimi Chat 等都是可选择的大模型组件，如果需要<strong>私有化部署，Deepseek、Qwen 系列、GLM 系列</strong>、Baichuan 系列都在可考虑范围。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250322190256861.png" alt="image-20250322190256861"></p>
<p>更多包含机构和学术背景且具有明确来源的大模型信息，可以访问实时更新的 <a href="https://github.com/wgwang/awesome-LLMs-In-China"><strong>中国大模型列表</strong></a> 全面记录中国大模型的发展动态。</p>
<h4 id="大模型原理">大模型原理</h4>
<p>一切始于 Google 在 2017 年发表的论文 <strong>《Attention Is All You Need》</strong>，引入了 <strong>Transformer</strong> 模型，它是深度学习领域的一个突破性架构，大型语言模型的成功得益于对 Transformer 模型的应用。</p>
<p>与传统的循环神经网络（RNN）相比，Transformer 模型<strong>不依赖于序列顺序</strong>，而是通过自注意力（Self-Attention）机制来捕捉序列中各元素之间的关系。Transformer 由多个堆叠的编码层（Encoder）和解码层（Decoder）组成，每一层包括自注意力层、前馈层和归一化层。这些层协同工作，逐步捕捉输入数据信息特征，从而<strong>预测输出</strong>，实现强大的语言理解和生成能力。</p>
<p>Transformer 模型的<strong>核心创新在于位置编码和自注意力机制</strong>。位置编码帮助模型理解输入数据的顺序信息，而自注意力机制则允许模型根据输入的全局上下文，为每个词元分配不同的注意力权重，从而更准确地理解词与词之间的关联性。这种机制使得 Transformer 特别适用于语言模型，因为语言模型需要精确捕捉上下文中的细微差别，生成符合语义逻辑的文本。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250322175522327.png" alt="image-20250322175522327"></p>
<p>上图展示了 Transformer 模型的架构及其核心机制的可视化示例。左图中，Transformer 模型由编码器和解码器两部分组成。编码器负责理解输入信息的顺序和语义，解码器则输出概率最高的词元。</p>
<p>右上图中的示例显示了输入句子中的填空任务，解码器依据输入句子的特征和已生成的部分句子，生成了“She”作为模型的预测结果。生成“She”的核心原因在于右下图所示的注意力机制，其中需要填空的部分对输入句子中的词元“The Doctor”和“Nurse”分配了较高的注意力权重，从而提高了“She”作为输出词元的生成概率。</p>
<p><strong>大语言模型的突破</strong>始于 2022 年年底 OpenAI 发布的 <strong>ChatGPT。其核心优势体现在庞大的参数规模（数百亿甚至数千亿）、基于 PB 级别数据的训练所带来的卓越语言理解与生成能力，以及其显著的涌现能力。</strong>  大语言模型不仅在传统的自然语言处理任务中展现了卓越表现，还具备了解决复杂问题和进行逻辑推理等高级认知能力。</p>
<h4 id="RAG中大模型的选择">RAG中大模型的选择</h4>
<p>在如今大模型层出不穷的情况下，如何在 RAG 应用场景中选择合适的模型呢？我们面对的是<strong>开源与闭源</strong>的选择、<strong>大参数与小参数</strong>的对比，成本的考虑以及<strong>云端与私有化部署</strong>的抉择。针对这些问题，我们需要结合测评和具体的应用场景进行综合考量。</p>
<p>从测评角度来看，前面已经介绍了<a href="https://www.cluebenchmarks.com/static/superclue.html"><strong>中文通用大模型的综合性测评基准 SuperCLUE</strong></a>，它对中文场景中的多个任务分支进行测试，涵盖基础能力、专业能力以及中文特性多个方面。每个任务分支又包含多个维度，例如语义理解、生成与创作、代数、生物、成语、诗词等。下图展示了这些维度的具体内容，SuperCLUE 每月都会更新测评结果，确保其反映大模型的最新表现。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250322180251002.png" alt="image-20250322180251002"></p>
<p>尤其需要关注的是 <a href="https://www.superclueai.com/"><strong>SuperCLUE-RAG 检索增强生成测评</strong></a>，在 RAG 场景中，大模型的检索能力表现是核心。SuperCLUE 针对 RAG 应用场景进行了独立测试，具体评估了大模型在检索和生成过程中的表现，测试数据如下图所示（最新为2024 年 11月数据）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250322181526225.png" alt="image-20250322181526225"></p>
<p>在 SuperCLUE 官网的 SuperCLUE-RAG 检索增强生成分支页面上，可以查看其总榜及四大基础任务的测评结果。选择模型时可以根据总分，以及模型在<strong>答案及时性、信息整合能力、拒答能力、检错和纠错能力</strong>等方面的表现，进行综合评估，作为场景选型参考。</p>
<p>其次，也是最重要的，我们需要根据<strong>实际应用场景</strong>来考量并选择适合的大模型，以下几个维度是关键：</p>
<p><strong>1. 开源与闭源</strong>：开源模型适用于<strong>数据敏感性</strong>高或有严格合规要求的场景，通过自托管实现对数据的完全掌控，确保<strong>隐私与安全</strong>。而闭源模型则适合数据敏感度较低的应用场景，其维护与服务相对完善，能够降低运维复杂度。</p>
<p><strong>2. 模型参数规模</strong>：大参数模型在复杂任务中的<strong>推理与生成能力</strong>较强，但并非所有应用场景都需要高精度模型。小参数模型（如 7B）在满足简单逻辑任务时，具备更优的<strong>响应速度、成本控制和资源利用</strong>效率。因此，模型规模应依据应用复杂性及算力预算进行合理匹配。</p>
<p><strong>3. 国内与国外部署</strong>：模型选择还需考虑部署环境。如果应用主要在国内进行，虽然调用国外大模型的接口是可行的，但可能会遇到<strong>稳定性、网络延迟</strong>、注册认证、充值付费等方面的实际问题。此外，数据合规性是重要考量，尤其对于需要遵循国内<strong>隐私和数据安全</strong>法规的场景，选择国内大模型或本地化部署更为合适。综上所述，模型的选择应结合 RAG 应用场景的需求和限制，更好地选择合适的大模型以最大化其效果。</p>
<p>选择建议：</p>
<p><strong>1. 闭源模型</strong>: 通义千问、文心一言、混元大模型、豆包大模型和 Kimi Chat 等旗舰模型实际差异不大，<strong>取决于成本</strong>。</p>
<p><strong>2. 开源模型</strong>: 根据需求可以在<strong>千问系列，deepseek系列，百川系列，GLM系列</strong>这几个中选择。</p>
<p><strong>参考</strong>：<a href="https://www.cluebenchmarks.com/superclue_2503"><strong>中文大模型基准评测2025年3月报告</strong></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>生成</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>生成</tag>
        <tag>大模型</tag>
        <tag>提示词</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG生成（二）：提示词工程</title>
    <url>/2025/03/23/RAG%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h4 id="提示词概述">提示词概述</h4>
<p><strong>提示词工程（Prompt Engineering）</strong>，旨在通过开发和优化提示来有效利用语言模型的潜力。</p>
<p>提示词工程是为生成式 AI 模型<strong>设计输入</strong>以获取最佳输出的实践。 <strong>这些输入被称为提示词（Prompt）</strong> 。</p>
<p>提示词工程<strong>核心理念</strong>是，通过提供更优质的输入，可以让生成式 AI 模型（如大型语言模型）生成更符合需求的结果，也就是<strong>让模型能够更好地执行各种任务</strong> ，包括生成创作，生成代码，聊天互动等。</p>
<span id="more"></span>
<h4 id="一个通用的提示词通常包含以下几个元素：">一个通用的提示词通常包含以下几个元素：</h4>
<p><strong>1. 指令（Instruction）：<strong>指明模型要</strong>执行的特定任务或操作</strong>。</p>
<p><strong>2. 上下文（Context）：<strong>为模型提供</strong>额外信息或背景</strong>，可以帮助引导模型生成更准确的响应。</p>
<p><strong>3. 输入数据（Input Data）：</strong> 我们希望模型回答的问题或感兴趣的输入内容。</p>
<p><strong>4. 输出指示符（Output Indicator）：<strong>指定模型的</strong>输出类型或格式</strong>，例如格式、是否要求生成代码、总结文本或回答具体问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250323075425612.png" alt="image-20250323075425612"></p>
<p>提示词工程不仅仅是设计提示，而是<strong>通过深刻理解模型的功能和局限性</strong>，创造能够与模型输入产生最佳互动的提示，这其中包含了一系列的技巧。</p>
<h4 id="RAG中提示词工程的技巧">RAG中提示词工程的技巧</h4>
<p>这些技巧就是解决RAG中的问题，如缺失内容，格式错误，缺乏细节，回答不全面等采取的具体措施, 也是对评测指标答案及时性、信息整合能力、拒答能力、检错和纠错能力所做的针对设计。</p>
<p><strong>1. 具体指令法</strong> （优化指令）</p>
<p>指定回答预期，防止内容缺失，回答不全面：</p>
<p>清晰的指令可以让模型输出：更<strong>明确的任务目标和更符合预期</strong>的内容。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">请根据上传的银行业报告，简洁总结当前的市场趋势，重点分析政策变化对行业的影响，输出为以下Markdown格式：</span><br><span class="line"><span class="bullet">-</span> <span class="strong">**市场趋势**</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**政策影响**</span></span><br><span class="line"><span class="bullet">-</span> <span class="strong">**竞争风险**</span></span><br></pre></td></tr></table></figure>
<p><strong>2 . 解释理由法</strong> （优化指令, 强调指令的原因）</p>
<p>提升模型对任务特定规则或背景的理解能力：</p>
<p>在编写提示时，向模型解释为什么某些任务需要特定的处理方式。这样可以帮助模型更好地理解任务背景，从而提高输出的质量和相关性。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">请生成一份简明扼要的银行业报告摘要，不要逐字重复段落内容。原因：读者可以访问完整文档，如果需要可以详细阅读全文。</span><br></pre></td></tr></table></figure>
<p><strong>3. 任务角色设定</strong> (优化上下文，指定模型身份、立场和责任)</p>
<p>提升模型专业领域回答能力：</p>
<p>通过为模型设定特定的角色身份，可以帮助模型更好地理解任务要求和角色责任，从而输出更加一致、专业的内容。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">你的角色: 知识库专家</span><br><span class="line"><span class="bullet">-</span> 背景：分析银行业市场数据</span><br><span class="line"><span class="bullet">-</span> 目标：生成一份详细的行业趋势分析</span><br><span class="line"><span class="bullet">-</span> 限制：仅根据报告中的数据生成分析</span><br></pre></td></tr></table></figure>
<p><strong>4. 文档基础说明</strong> （优化上下文，对输入数据提供背景和来源信息）</p>
<p>奠定模型任务推理的基调：</p>
<p>为模型提供文档的背景信息和文本来源可以帮助奠定任务基础，让模型更好地进行任务推理和回答。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">以下是关于银行业政策变化的相关规则，它们将用于回答有关政策对银行业影响的问题。</span><br></pre></td></tr></table></figure>
<p><strong>5. 示例学习</strong> （优化输出指示符）</p>
<p>优化回答细节以及格式：</p>
<p>通过给模型提供多个<strong>参考示例</strong>，模型可以基于这些示例进行模式识别，进而<strong>模仿、思考并生成类似的答案</strong>。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">以下是两个关于银行业的分析示例，请按照这种格式对新的报告进行分析：</span><br><span class="line"><span class="bullet">-</span> 示例 1：<span class="strong">**市场趋势**</span>：由于政策放宽，银行贷款增长迅速。</span><br><span class="line"><span class="bullet">-</span> 示例 2：<span class="strong">**政策影响**</span>：新的利率政策可能会对中小企业贷款产生负面影响。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">请对下面报告进行同样的分析。</span><br></pre></td></tr></table></figure>
<p><strong>6. 默认回复策略</strong> （放在指令，上下文或者输出指示符部分都可以，旨在强调基于文档回答）</p>
<p>提升模型的拒答能力：</p>
<p>当模型无法从文档中获取足够信息时，通过设定默认回复策略，避免模型产生“幻觉”，即生成虚假的答案。这可以确保模型仅基于文档中的事实进行回答。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">如果文档中没有足够的事实回答问题，请返回&#123;无法从文档中获得相关内容&#125;，而不是进行推测。</span><br></pre></td></tr></table></figure>
<p>通过以上提示工程技巧，RAG 任务的输出质量可以高效、低成本地提升，解决常见生成问题。</p>
<h4 id="写在最后：">写在最后：</h4>
<p>在提示工程中，过于具体的指令可能会限制模型的创造性，过于宽泛的提示则可能导致生成偏差。</p>
<p>如何在提示设计中找到合适的权衡点，既能够引导模型生成高质量结果，又不过度限制模型的灵活性，需要大家在实际场景中探索。</p>
<p><strong>进阶：</strong> 深入理解模型提示词的原理</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250323104447095.png" alt="image-20250323104447095"></p>
<p><strong>突破路径：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250323104703182.png" alt="image-20250323104703182"></p>
<p>提示词参考阅读：</p>
<p><a href="https://pan.baidu.com/s/12hTcfYD3qDir_-_SSr9yxw?pwd=cghq">DeepSeek：从入门到精通（点开是百度网盘链接，放在书籍&amp;手册目录下）</a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>生成</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>生成</tag>
        <tag>提示词</tag>
        <tag>generate</tag>
      </tags>
  </entry>
  <entry>
    <title>Manus 首款通用AI智能体</title>
    <url>/2025/03/07/Manus-%E9%A6%96%E6%AC%BE%E9%80%9A%E7%94%A8AI%E6%99%BA%E8%83%BD%E4%BD%93/</url>
    <content><![CDATA[<h1>做AI场景的知识分享，尤其是涉及头部的智能体平台，我前面主要介绍了Dify的使用，这里就不得不提这两天各种关于 Manus 炸裂的新闻。</h1>
<ul>
<li>
<h2 id="首先说清楚，我没拿到邀请码，申请的时候网站还崩了，所以我在官网看了下效果并查看了其他博主演示视频。">首先说清楚，我没拿到邀请码，申请的时候网站还崩了，所以我在官网看了下效果并查看了其他博主演示视频。</h2>
</li>
<li>
<h3 id="说说我的想法">说说我的想法</h3>
<ul>
<li>
<h3 id="第一点，Manus如果公测或者正式发布，效果不打折扣，那么它确实能从本质上帮助普通用户几乎没有门槛的使用AI技术。带来的可能是很多领域将实现技术平权。">第一点，Manus如果公测或者正式发布，效果不打折扣，那么它确实能从本质上帮助普通用户几乎没有门槛的使用AI技术。带来的可能是很多领域将实现技术平权。</h3>
</li>
<li>
<h3 id="再来说第二点，很快国内的大厂应该就会推出同类产品，市场将出现Agent技术的一次跃升。">再来说第二点，很快国内的大厂应该就会推出同类产品，市场将出现Agent技术的一次跃升。</h3>
<ul>
<li>从目前头部的几个集成平台说起：</li>
<li><strong>字节的扣子</strong>
<ul>
<li>积累了大量的开发者和插件，再加上火山引擎算力支持，完全有能力实现弯道超车。</li>
</ul>
</li>
<li><strong>Dify</strong><br>
一个可以私有化部署的开源框架，已经有很多企业和个人搭建了自己的AI平台，整合出一个通用Agent也是早晚得事。</li>
<li>再者还有腾讯的元宝或者说元器平台也是有机会的。</li>
<li>这里面因为Manus这种产品形态对算力要求太高了，所以大厂有资源不会放过这个机会。</li>
</ul>
</li>
</ul>
</li>
</ul>
<span id="more"></span>
<ul>
<li>
<h2 id="最后说下Manus的效果和启发：">最后说下Manus的效果和启发：</h2>
<ul>
<li>
<h4 id="Manus就像一个能力超群的全能助手，简单点就是：一句话他就可以接管你的电脑为你做：">Manus就像一个能力超群的全能助手，简单点就是：一句话他就可以接管你的电脑为你做：</h4>
<ul>
<li>商业研究</li>
<li>生活帮手</li>
<li>数据分析</li>
<li>教育可视化</li>
<li>以及各种提升生产率的工具</li>
</ul>
</li>
<li>
<p><strong>生活案例</strong>：规划4月去日本旅行(<strong>执行过程中还创建了日本旅游手册</strong>)</p>
<ul>
<li><a href="https://hfhfn.github.io/image_storage/html/%E6%97%A5%E6%9C%AC%E6%97%85%E6%B8%B8%E6%89%8B%E5%86%8C.html">日本旅游手册</a></li>
</ul>
</li>
<li>
<p><strong>生活分析案例</strong>：分析特斯拉的股票（<strong>创建了分析仪表板面</strong>）</p>
<ul>
<li><a href="https://pljclduq.manus.space/">特斯拉股票分析仪表板</a></li>
</ul>
</li>
<li>
<p><strong>教育展示案例</strong>：生成互动课程：<strong>动量定理</strong></p>
<ul>
<li><a href="https://hfhfn.github.io/image_storage/html/conservation_momentum/index.html">互动课程：动量定理</a></li>
</ul>
</li>
<li>
<p><strong>数据分析案例</strong>：雪松-西奈急救中心的人口和疾病患病率（<strong>只展示它分析中生成的图表，看看这图表的炸裂效果</strong>）</p>
<ul>
<li>
<div style="display: flex;">
  <img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/disease_prevalence_comparison.png" alt="Image 1" style="width: 50%;">
  <img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/disease_prevalence_chart.png" alt="Image 2" style="width: 50%;">
</div>
</li>
<li>
<div style="display: flex;">
  <img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/population_distribution.png" alt="Image 1" style="width: 50%;">
  <img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/neighborhood_disease_heatmap.png" alt="Image 2" style="width: 50%;">
</div>
</li>
<li>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/disease_by_neighborhood_3d.png" alt="disease_by_neighborhood_3d"></p>
</li>
</ul>
</li>
<li>
<h4 id="启发1：我观察到Manus执行任务时，它会在网络上进行大规模的信息检索，但它能力再强，海量的数据造成的AI幻觉和AI偏见会变得越来越难以避免。">启发1：我观察到Manus执行任务时，它会在网络上进行大规模的信息检索，但它能力再强，海量的数据造成的AI幻觉和AI偏见会变得越来越难以避免。</h4>
</li>
<li>
<h4 id="启发2：-或许我们不一定要去搭建自己的一个智能体，而是积累我们的大量知识库内容，等待大厂的通用Agent到来时，反而直接会被赋予更大的活力，真正实现更智能的交互和应用。">启发2： 或许我们不一定要去搭建自己的一个智能体，而是积累我们的大量知识库内容，等待大厂的通用Agent到来时，反而直接会被赋予更大的活力，真正实现更智能的交互和应用。</h4>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Agent</tag>
        <tag>Manus</tag>
        <tag>智能体</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG索引（一）：文档解析技术</title>
    <url>/2025/03/07/RAG%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h1>RAG索引流程中面临的文档解析任务</h1>
<h2 id="1-文档解析现状">1. 文档解析现状</h2>
<ul>
<li>文档解析技术的本质在于将格式各异、版式多样、元素多种的文档数据，包括段落、表格、标题、公式、多列、图片等文档区块，转化为阅读顺序正确的字符串信息。</li>
<li>高质量的文档解析能够从各种复杂格式的非结构化数据中提取出高精准度的信息，对 RAG 系统最终的效果起决定性的作用。</li>
<li>RAG应用场景中涉及的数据类型通常有：PDF、TXT、Word、PPT、Excel、CSV、Markdown、XML、HTML以及关系型和非关系型数据库等，这里面最常见也是最难的就是PDF的解析。</li>
<li>PDF 文档往往篇幅巨大、页数众多，且企业及专业领域 PDF 文件数据量庞大，因此文档解析技术还需具备极高的处理性能，以确保知识库的高效构建和实时更新。</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250307075848111.png" alt="image-20250307075848111"></li>
</ul>
<span id="more"></span>
<h2 id="2-文档解析工具选择（这里主要讨论开源工具）">2. 文档解析工具选择（这里主要讨论开源工具）</h2>
<ul>
<li>
<h3 id="我们可以把所有工具分为两大类">我们可以把所有工具分为两大类</h3>
<ul>
<li>一个是<strong>基于规则</strong>的开源库</li>
<li>一个是<strong>基于深度学习</strong>的开源库</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250307080819281.png" alt="image-20250307080819281"></li>
</ul>
</li>
<li>
<h4 id="这里我们首先介绍基于规则的解析工具">这里我们首先介绍基于规则的解析工具</h4>
<ul>
<li>因为我们选择的框架langchain提供在实际应用场景中常见文档格式基于规则的解析方案，涵盖 PDF、TXT、Word、PPT、Excel、CSV、Markdown、XML 和 HTML 格式。</li>
<li>所以初步使用<strong>langchain</strong>框架LangChain Document Loaders 文档加载器进行操作，也就是<strong>langchain_community.document_loaders</strong>模块。</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250307081912468.png" alt="image-20250307081912468"></li>
<li>以下是langchain Document Loader 所需的文档解析依赖库</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250307082205941.png" alt="image-20250307082205941"></li>
<li>例如，当我们项目中使用 from langchain_community.document_loaders import PDFPlumberLoader 时，需要先通过命令行 pip install pdfplumber 安装 pdfplumber 库。某些特殊情况下，还需要额外的依赖库，比如使用 UnstructuredMarkdownLoader 时，需要安装 unstructured 库来提供底层文档解析，还需要 markdown 库来支持 Markdown 文档格式更多能力。此外，对于像 .doc 这种早期的文档类型，还需要安装 libreoffice 软件库才能进行解析。</li>
<li>针对PDF文档，目前主流是转为MarkDown文件格式。PDF 文件分为电子版和扫描版，**PDF 电子版可以通过规则解析，提取出文本、表格等文档元素。**目前，有许多开源库可以支持，例如 pyPDF2、PyMuPDF、pdfminer、pdfplumber 和 papermage 等。这些库在 langchain_community.document_loaders 中基本都有对应的加载器。</li>
<li><strong>在基于规则的开源库中，pdfplumber 对中文支持较好，且在表格解析方面表现优秀，但对双栏文本的解析能力较差；pdfminer 和 PyMuPDF 对中文支持良好，但表格解析效果较弱；pyPDF2 对英文支持较好，但中文支持较差；papermage 集成了 pdfminer 和其他工具，特别适合处理论文场景。开发者可以根据实际业务场景的测试结果选择合适的工具，pdfplumber 或 pdfminer 都是当前不错的选择。</strong></li>
</ul>
</li>
<li>
<h4 id="再来看基于深度学习的开源解析工具">再来看基于深度学习的开源解析工具</h4>
<ul>
<li>基于深度学习的工具基本都是因PDF而生，无论扫描版还是电子版均需进行版面分析和阅读顺序的还原，将内容解析为一个包含所有文档元素并且具有正确阅读顺序的 MarkDown 文件。单纯依赖规则解析是无法实现这一目标的，目前支持这些功能的多为基于深度学习的开源库，如 Layout-parser、PP-StructureV2、PDF-Extract-Kit、pix2text、MinerU、marker 等。</li>
<li>由于深度学习模型的部署复杂性以及对显卡配置的要求，langchain目前基本集成的都是基于规则的解析工具，基于深度学习的工具需要进行独立部署。</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250307083713327.png" alt="image-20250307083713327"></li>
<li>除了以上提到的还有比如：RapidLayout 专注于版面分析的，以及众多专注OCR（光学字符识别）识别的ZeroX，GOT-OCR2.0, OCRmyPDF，olmOCR等。</li>
</ul>
</li>
<li>
<p>目前除了这些工具，采用 <strong>端到端的多模态大模型直接解析</strong> 包括文字模态以及图片中非文字内容的解析，如常见的折线图、柱状图等，也许是文档解析的终极形式，但模型在效率和成本方面仍存在挑战，但其未来潜力巨大，值得期待。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>索引</category>
        <category>文档解析</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>文档解析</tag>
        <tag>langchain_community.document_loaders</tag>
        <tag>pdfplumber</tag>
        <tag>unstructured</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG索引（三）：嵌入（Embedding）技术</title>
    <url>/2025/03/10/RAG%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%B5%8C%E5%85%A5%EF%BC%88Embedding%EF%BC%89%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h3 id="嵌入模型（Embedding-Model）负责将文本数据映射到高维向量空间中，将输入的文档片段转换为对应的嵌入向量（embedding-vectors）。这些向量捕捉了文本的语义信息，并被存储在向量库（VectorStore）中，以便后续检索使用。用户查询（Query）同样通过嵌入模型的处理生成查询嵌入向量，这些向量用于在向量数据库中通过向量检索（Vector-Retrieval）匹配最相似的文档片段。根据不同的场景需求，评估并选择最优的嵌入模型，以确保-RAG-的检索性能符合要求。"><mark>嵌入模型（Embedding Model）负责将文本数据映射到高维向量空间中</mark>，将输入的文档片段转换为对应的嵌入向量（embedding vectors）。这些向量捕捉了文本的语义信息，并被存储在向量库（VectorStore）中，以便后续检索使用。<mark>用户查询（Query）同样通过嵌入模型的处理生成查询嵌入向量</mark>，这些向量用于在向量数据库中通过向量检索（Vector Retrieval）匹配最相似的文档片段。根据不同的场景需求，<mark>评估并选择最优的嵌入模型</mark>，以确保 RAG 的检索性能符合要求。</h3>
<span id="more"></span>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E4%BD%9C%E7%94%A8.png" alt="嵌入模型作用" style="zoom: 67%;" />
<ul>
<li>
<h4 id="什么是Embedding嵌入？">什么是Embedding嵌入？</h4>
<ul>
<li>Embedding 嵌入是指将文本、图像、音频、视频等形式的信息映射为高维空间中的密集向量表示。这些向量在语义空间中起到坐标的作用，捕捉对象之间的语义关系和隐含的意义。通过在向量空间中进行计算（例如余弦相似度），可以量化和衡量这些对象之间的<strong>语义相似性</strong>。</li>
<li>在具体实现中，嵌入的每个维度通常对应文本的某种特征，例如性别、类别、数量等。通过多维度的数值表示，计算机能够理解并解析文本的复杂语义结构。例如，“man”和“woman”在描述性别维度上具有相似性，而“king”和“queen”则在性别和王室身份等维度上表现出相似的语义特征。</li>
<li>向量是一组在高维空间中定义点的数值数组，而<strong>嵌入则是将信息（如文本）转化为这种向量表示的过程</strong>。这些向量能够捕捉数据的语义及其他重要特征，使得语义相近的对象在向量空间中彼此邻近，而语义相异的对象则相距较远。<strong>向量检索（Vector Retrieval）是一种基于向量表示的搜索技术</strong>，通过计算查询向量与已知文本向量的相似度来识别最相关的文本数据。向量检索的高效性在于，它能在大规模数据集中快速、准确地找到与查询最相关的内容，这得益于向量表示中蕴含的丰富语义信息。</li>
<li>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250310112108258.png" alt="image-20250310112108258" style="zoom: 80%;" />
</li>
</ul>
</li>
<li>
<h4 id="Embedding-Model-嵌入模型">Embedding Model 嵌入模型</h4>
<ul>
<li>自 2013 年以来，word2vec、GloVe、fastText 等嵌入模型通过分析大量文本数据，学习得出单词的嵌入向量。近年来，随着 transformer 模型的突破，嵌入技术以惊人的速度发展。BERT、RoBERTa、ELECTRA 等模型将词嵌入推进到上下文敏感的阶段。这些模型在为文本中的每个单词生成嵌入时，会充分考虑其上下文环境，因此同一个单词在不同语境下的嵌入向量可以有所不同，从而大大提升了模型理解复杂语言结构的能力。</li>
<li>在 RAG 系统中，Embedding Model 嵌入模型扮演着关键角色，负责将文本数据映射到高维向量空间，以便高效检索和处理。具体而言，<strong>Embedding Model 将输入的文档片段（Chunks）和查询文本（Query）转换为嵌入向量（Vectors）</strong>，这些向量捕捉了文本的语义信息，并可在向量空间中与其他嵌入向量进行比较。</li>
<li>在 RAG 流程中，文档首先被分割成多个片段，每个片段随后通过 Embedding Model 进行嵌入处理。生成的文档嵌入向量被存储在 VectorStore 中，供后续检索使用。用户查询会通过 Embedding Model 转换为查询嵌入向量，这些向量用于在向量数据库中匹配最相似的文档片段，最终组合生成指令（Prompt），大模型生成回答。</li>
<li>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250310112648314.png" alt="image-20250310112648314" style="zoom:80%;" />
</li>
<li>正如图中所示，嵌入模型是 RAG 流程的核心。既然如此重要，市面上有非常多的嵌入模型，我们该如何为我们的业务场景选择最合适的嵌入模型呢？</li>
</ul>
</li>
<li>
<h4 id="Embedding-Model-嵌入模型评估与选择">Embedding Model 嵌入模型评估与选择</h4>
<ul>
<li>在选择适合的嵌入模型时，需要综合考虑多个因素，包括<strong>特定领域的适用性、检索精度、支持的语言、文本块长度、模型大小以及检索效率</strong>等因素。同时以广泛受到认可的 **MTEB（Massive Text Embedding Benchmark）和 C-MTEB（Chinese Massive Text Embedding Benchmark）**榜单作为参考，通过涵盖分类、聚类、语义文本相似性、重排序和检索等多个数据集的评测，开发者可以根据不同任务的需求，评估并选择最优的向量模型，以确保在特定应用场景中的最佳性能。</li>
<li>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250310113405164.png" alt="image-20250310113405164" style="zoom:80%;" />
</li>
<li><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB &amp; C-MTEB 榜单</a></li>
<li>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250310122014515.png" alt="image-20250310122014515" style="zoom:80%;" />
</li>
<li>榜单每日更新，可以切换语言为 中文，可以看到中文嵌入模型的排名。由于 RAG 是一项检索任务，我们需要按“Retrieval Average”（检索平均值）列对排行榜进行排序，图中显示的就是检索任务效果排序后的结果。在检索任务中，我们需要在榜单顶部看到最佳的检索模型，并且专注于以下几个关键列：</li>
<li><strong>Retrieval Average 检索平均值</strong>：较高的检索平均值表示模型更擅长在检索结果列表中将相关项目排在较高的位置，检索效果更好。</li>
<li><strong>Model Size 模型大小</strong>：模型的大小（以 GB 为单位）。虽然检索性能随模型大小而变化，但要注意，模型大小也会对延迟产生直接影响。因此，在选择模型时，建议筛选掉那些在硬件资源有限的情况下不可行的过大模型。在生产环境中，性能与效率之间的权衡尤为重要。</li>
<li><strong>Max Tokens 最大 Token 数</strong>：可压缩到单个文本块中的最大 Token 数。因为文档块我们希望不要过大而降低目标信息块的精准度，因此，即使最大 tokens 数为 512 的模型在大部分场景下也足够使用。</li>
<li><strong>Embedding Dimensions：嵌入向量的维度</strong>。越少的嵌入维度提供更快的推理速度，存储效率更高，而更多的维度可以捕获数据中的细微特征。我们需要在模型的性能和效率之间取得良好的权衡。</li>
<li><strong>实验至关重要</strong>，在排行榜上表现良好的模型不一定在你的任务上表现良好，试验各种高得分的模型至关重要。我们参考 MTEB 排行榜，选择多个适合我们场景的嵌入模型作为备选，并在我们的业务场景数据集上进行评估测试，以选出最适合我们 RAG 系统的嵌入模型。</li>
</ul>
</li>
<li>
<h4 id="Embedding-Model-技术实战">Embedding Model 技术实战</h4>
<ul>
<li>
<p>我们可以使用 SentenceTransformers 作为加载嵌入模型的 Python 模块。</p>
</li>
<li>
<p>SentenceTransformers（又名 SBERT）是一个用于训练和推理文本嵌入模型的 Python 模块，可以在 RAG 系统中计算嵌入向量。使用 SentenceTransformers 进行文本嵌入转换非常简单：只需导入模块库、加载模型，并调用 encode 方法即可。执行时，SentenceTransformers 会自动下载相应的模型库，当然也可以手动下载并指定模型库的路径。所有可用的模型都可以在 <a href="https://www.sbert.net/docs/sentence_transformer/pretrained_models.html">SentenceTransformers 模型库</a> 查看，超过 8000 个发布在 Hugging Face 上的嵌入模型库可以被使用。</p>
</li>
<li>
<p>在中文领域，智源研究院的 <a href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d">BGE 系列模型</a> 是较为知名的开源嵌入模型，在 C-MTEB 上表现出色。BGE 系列目前包含 23 个嵌入模型，涵盖多种维度、多种最大 Token 数和模型大小，用户可以根据需求进行测试和使用。</p>
</li>
<li>
<p><strong>load_embedding_model 方法中使用 SentenceTransformer 加载嵌入模型代码</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绝对路径：SentenceTransformer读取绝对路径下的bge-large-zh-v1.5模型，如需使用其他模型，下载其他模型，并且更换绝对路径即可</span></span><br><span class="line">embedding_model = SentenceTransformer(os.path.abspath(<span class="string">&#x27;data/bge-large-zh-v1.5&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动下载：SentenceTransformer库自动下载BAAI/bge-large-zh-v1.5模型，如需下载其他模型，输入其他模型名称即可</span></span><br><span class="line"><span class="comment"># embedding_model = SentenceTransformer(&#x27;BAAI/bge-large-zh-v1.5&#x27;)</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>indexing_process 方法中将文本转化为嵌入向量代码</strong>：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 文本块转化为嵌入向量列表，normalize_embeddings表示对嵌入向量进行归一化，用于后续流程准确计算向量相似度</span></span><br><span class="line">embeddings = []</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> all_chunks:</span><br><span class="line">      embedding = embedding_model.encode(chunk, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">      embeddings.append(embedding)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<h4 id="总结">总结</h4>
<ul>
<li>嵌入技术将文本数据映射到高维向量空间中，捕捉其语义信息，支持向量检索，从而在大规模数据中快速识别与查询最相关的文档片段。在选择嵌入模型时，需要综合考虑特定领域的适用性、检索精度、支持的语言、文本块长度、模型大小以及检索效率等因素。</li>
<li>通过参考 MTEB 和 C-MTEB 的评测榜单，可以评估多个高得分的模型，并在具体的业务场景中进行测试，最终选择最适合该场景的嵌入模型。同时，使用 SentenceTransformers Python 模块可以简化嵌入模型的加载和嵌入计算，进而高效率集成测试。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>索引</category>
        <category>嵌入（Embedding）技术</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>索引</tag>
        <tag>嵌入</tag>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG索引（二）：分块策略</title>
    <url>/2025/03/09/RAG%E7%B4%A2%E5%BC%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<h3 id="文档数据（Documents）经过解析后，通过分块技术将信息内容划分为适当大小的文档片段（chunks），从而使-RAG-系统能够高效处理和精准检索这些片段信息。">文档数据（Documents）经过解析后，通过分块技术将信息内容划分为适当大小的文档片段（chunks），从而使 RAG 系统能够高效处理和精准检索这些片段信息。</h3>
<h3 id="选择适合特定场景的分块策略是提升-RAG-系统召回率的关键。">选择适合特定场景的分块策略是提升 RAG 系统召回率的关键。</h3>
<ul>
<li>
<h4 id="为什么说分块很重要？">为什么说分块很重要？</h4>
<ul>
<li>
<p>分块的目标在于确保每个片段在<strong>保留核心语义</strong>的同时，具备<strong>相对独立的语义完整性</strong>，从而使模型在处理时不必依赖广泛的上下文信息，<strong>增强检索召回的准确性</strong>。</p>
</li>
<li>
<p><strong>分块的重要性在于</strong>它直接影响 RAG 系统的生成质量。首先，合理的分块能够确保<strong>检索到的片段与用户查询信息高度匹配</strong>，避免信息冗余或丢失。</p>
</li>
<li>
<p><strong>好的分块有助于提升生成内容的连贯性</strong>，精心设计的独立语义片段可以<strong>降低模型对上下文的依赖</strong>，从而增强生成的逻辑性与一致性。</p>
</li>
<li>
<p>分块策略的选择还会<strong>影响系统的响应速度与效率</strong>，模型能够更快、更准确地处理和生成内容。</p>
</li>
<li>
<h4 id="分块策略最大的挑战在于确定分块的大小。">分块策略最大的挑战在于确定分块的大小。</h4>
<span id="more"></span>
<ul>
<li>如果<strong>片段过大</strong>，可能导致向量<strong>无法精确捕捉内容的特定细节</strong>并且计算成本增加；</li>
<li>若<strong>片段过小</strong>，则可能丢失上下文信息，导致<strong>句子碎片化和语义不连贯</strong>。</li>
<li>较小的块适用于需要细粒度分析的任务，例如<strong>情感分析</strong>，能够精确捕捉特定短语或句子的细节。</li>
<li>更大的块则更为合适需要保留更广泛上下文的场景，例如<strong>文档摘要或主题检测</strong>。</li>
</ul>
</li>
<li>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250309012438108.png" alt="image-20250309012438108"></p>
</li>
</ul>
</li>
<li>
<h4 id="分块策略">分块策略</h4>
<ul>
<li>
<p>多种分块策略从本质上来看，由以下三个关键组成部分构成：</p>
<ul>
<li><strong>大小</strong>：每个文档块所允许的最大字符数。</li>
<li><strong>重叠</strong>：在相邻数据块之间，重叠字符的数量。</li>
<li><strong>拆分</strong>：通过段落边界、分隔符、标记，或语义边界来确定块边界的位置。</li>
</ul>
</li>
<li>
<p>上述三个组成部分共同决定了分块策略的特性及其适用场景。基于这些组成部分，常见的分块策略包括：</p>
<ul>
<li><strong>固定大小分块（Fixed Size Chunking）、重叠分块（Overlap Chunking）、递归分块（Recursive Chunking）、文档特定分块（Document Specific Chunking）、语义分块（Semantic Chunking）、混合分块（Mix Chunking）</strong>。下面我将对这些策略逐一进行介绍。</li>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250309013703174.png" alt="image-20250309013703174"></li>
<li>我们可以通过<strong>分块可视化</strong>来看一下：（Chunk 切分可视化呈现链接: <a href="https://chunkviz.up.railway.app/%EF%BC%89">https://chunkviz.up.railway.app/）</a></li>
<li>固定大小分块（Fixed Size Chunking）
<ul>
<li>最基本的方法是将文档按固定大小进行分块，通常作为分块策略的基准线使用。</li>
<li>适用场景：适用于格式和大小相似的同质数据集，如新闻文章或博客文章。</li>
<li>问题：可能在句子或段落<strong>中断内容</strong>，导致无意义的文本块，缺乏灵活性，无法适应文本的自然结构。</li>
</ul>
</li>
<li>重叠分块（Overlap Chunking）
<ul>
<li>通过滑动窗口技术切分文本块，使新文本块与前一个块的<strong>内容部分重叠</strong>，从而<strong>保留块边界处的重要上下文信息</strong>，增强系统的语义相关性。</li>
<li>适用场景：需要深入理解语义并保持上下文完整性的文档，如法律文档、技术手册或科研论文。</li>
<li>问题： 增加冗余信息的存储，处理效率降低。</li>
</ul>
</li>
<li>递归分块（Recursive Chunking）
<ul>
<li>通过预定义的文本分隔符（如换行符\n\n、\n ，句号、逗号、感叹号、空格等）迭代地将文本分解为更小的块，以实现段大小的均匀性和语义完整性。此过程中，文本首先按较大的逻辑单元分割（如段落 \n\n），然后逐步递归到较小单元（如句子 \n 和单词），确保<strong>在分块大小限制内保留最强的语义片段</strong>。</li>
<li>适用场景： 这种方法适用于需要<strong>逐层分析</strong>的文本文档或需要分解成长片段、长段落的长文档，如研究报告、法律文档等。</li>
<li>问题： 不过仍有可能在块边界处模糊语义，容易将完整的语义单元切分开。</li>
</ul>
</li>
<li>文档特定分块（Document Specific Chunking）
<ul>
<li>根据文档的格式（如 Markdown、Latex、或编程语言如 Python 等）进行<strong>定制化</strong>分割的技术。</li>
<li>适用场景： 这种方法可以根据特定的文档结构，进行准确的语义内容切分，在编程语言、Markdown、Latex 等结构文档中表现出色。</li>
<li>问题： 但文档特定分块的方式格式依赖性强，不同格式之间的分块策略不通用，并且无法处理格式不规范及混合多种格式的情况。</li>
</ul>
</li>
<li>语义分块（Semantic Chunking）
<ul>
<li>基于文本的自然语言边界（如句子、段落或主题中断）进行分段的技术，需要使用 <strong>NLP 技术根据语义分词分句</strong>，旨在确保每个分块都包含语义连贯的信息单元。</li>
<li>常用的分块策略有 spaCy 和 NLTK 的 NLP 库，spaCy 适用于需要<strong>高效、精准语义切分</strong>的大规模文本处理，NLTK 更<strong>适合教学、研究和需要灵活自定义</strong>的语义切分任务。</li>
<li>适用场景：提高检索结果的相关性和准确性；复杂文档和上下文敏感的精细化分析。</li>
<li>问题：需要额外的计算资源，处理效率较低。</li>
</ul>
</li>
<li>混合分块（Mix Chunking）
<ul>
<li>综合利用不同分块技术的优势，提高分块的精准性和效率。</li>
<li>根据实际业务场景，设计多种分块策略的混合，能够灵活适应各种需求，提供更强大的分块方案。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>上述分块策略在 langchain_text_splitters 库中对应的具体方法类如下：</p>
<ul>
<li>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250309021222817.png" alt="image-20250309021222817"></p>
</li>
<li>
<p>SpacyTextSplitter 和 NLTKTextSplitter 需要额外安装 Python 依赖库，其中 SpacyTextSplitter 还需要按照文档的语言对应安装额外的语言模型。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate rag  <span class="comment"># 激活虚拟环境</span></span><br><span class="line">pip install spacy nltk -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">python -m spacy download zh_core_web_sm <span class="comment"># 如果需要进行中文分块，安装spacy中文语言模型</span></span><br><span class="line">python -m spacy download en_core_web_sm <span class="comment"># 如果需要进行英文分块，安装spacy英文语言模型</span></span><br></pre></td></tr></table></figure>
<ul>
<li>导入 langchain.text_splitter 中各种文档分块类代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> (</span><br><span class="line">        CharacterTextSplitter,</span><br><span class="line">        RecursiveCharacterTextSplitter,</span><br><span class="line">        MarkdownTextSplitter,</span><br><span class="line">        PythonCodeTextSplitter,</span><br><span class="line">        LatexTextSplitter,</span><br><span class="line">        SpacyTextSplitter,</span><br><span class="line">        NLTKTextSplitter</span><br><span class="line">	) <span class="comment"># 从 langchain.text_splitter 模块中导入各种文档分块类</span></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>CharacterTextSplitter、RecursiveCharacterTextSplitter、MarkdownTextSplitter、PythonCodeTextSplitter、LatexTextSplitter、NLTKTextSplitter 替换原有 text_splitter 参数的赋值类即可。</p>
</li>
<li>
<p>需要额外处理的是 SpacyTextSplitter，需要参数 pipeline 指定具体的语言模型才可以运行。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置SpacyTextSplitter分割文本块库</span></span><br><span class="line">text_splitter = SpacyTextSplitter(chunk_size=<span class="number">512</span>, chunk_overlap=<span class="number">128</span>, pipeline=<span class="string">&quot;zh_core_web_sm&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>索引</category>
        <category>分块策略</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>langchain</tag>
        <tag>索引</tag>
        <tag>AI</tag>
        <tag>分块策略</tag>
        <tag>Chunk</tag>
      </tags>
  </entry>
  <entry>
    <title>Text2SQL应用对比</title>
    <url>/2025/03/06/Text2SQL%E5%BA%94%E7%94%A8%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>Postgres.new 和 Vanna 是两个不同的工具，分别用于不同的场景和目的。以下是它们的对比：</p>
<hr>
<span id="more"></span>
<h3 id="1-Postgres-new"><strong>1. Postgres.new</strong></h3>
<ul>
<li><strong>类型</strong>：在线工具</li>
<li><strong>用途</strong>：快速创建一个临时的 PostgreSQL 数据库实例，用于测试、演示或学习。</li>
<li><strong>特点</strong>：
<ul>
<li>无需安装或配置，直接在浏览器中使用。</li>
<li>提供临时的数据库实例，通常有时间限制（例如 1 小时）。</li>
<li>适合快速测试 SQL 查询、数据库设计或演示。</li>
<li>支持标准的 PostgreSQL 功能。</li>
</ul>
</li>
<li><strong>适用场景</strong>：
<ul>
<li>快速测试 SQL 查询。</li>
<li>教学或演示 PostgreSQL 功能。</li>
<li>临时需要数据库环境的场景。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-Vanna"><strong>2. Vanna</strong></h3>
<ul>
<li><strong>类型</strong>：AI 驱动的工具</li>
<li><strong>用途</strong>：通过自然语言生成 SQL 查询，帮助用户与数据库交互。</li>
<li><strong>特点</strong>：
<ul>
<li>基于 AI 模型（如 OpenAI 的 GPT）生成 SQL。</li>
<li>支持自然语言输入，用户可以用简单的语言描述查询需求。</li>
<li>可以连接到多种数据库（如 PostgreSQL、MySQL、Snowflake 等）。</li>
<li>提供 Python 库，方便集成到数据工作流中。</li>
</ul>
</li>
<li><strong>适用场景</strong>：
<ul>
<li>非技术用户需要查询数据库。</li>
<li>快速生成复杂 SQL 查询。</li>
<li>数据分析师或开发人员希望提高工作效率。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="对比总结"><strong>对比总结</strong></h3>
<table>
<thead>
<tr>
<th>特性</th>
<th>Postgres.new</th>
<th>Vanna</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>类型</strong></td>
<td>在线临时数据库工具</td>
<td>AI 驱动的 SQL 生成工具</td>
</tr>
<tr>
<td><strong>主要用途</strong></td>
<td>创建临时 PostgreSQL 实例</td>
<td>通过自然语言生成 SQL 查询</td>
</tr>
<tr>
<td><strong>用户群体</strong></td>
<td>开发者、测试人员、学习者</td>
<td>数据分析师、非技术用户、开发者</td>
</tr>
<tr>
<td><strong>技术依赖</strong></td>
<td>无需 AI，纯数据库工具</td>
<td>依赖 AI 模型（如 GPT）</td>
</tr>
<tr>
<td><strong>集成能力</strong></td>
<td>无</td>
<td>支持 Python 库，可集成到工作流</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>测试、演示、学习</td>
<td>数据分析、快速生成 SQL、简化查询</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="选择建议"><strong>选择建议</strong></h3>
<ul>
<li>如果你需要快速创建一个临时的 PostgreSQL 数据库实例进行测试或学习，选择 <strong>Postgres.new</strong>。</li>
<li>如果你希望通过自然语言生成 SQL 查询，简化与数据库的交互，选择 <strong>Vanna</strong>。</li>
</ul>
<p>两者可以结合使用：例如，用 Postgres.new 创建一个临时数据库，然后用 Vanna 生成 SQL 查询并执行。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>Text2SQL</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>AI工具</tag>
        <tag>Text2SQL</tag>
        <tag>自然语言操作数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Text2SQL的应用</title>
    <url>/2025/04/04/Text2SQL%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h3 id="基本原理剖析">基本原理剖析</h3>
<p><strong>Text2SQL 的实现就是基于 RAG 技术</strong> ，根据 Query 内容，从数据库中查询与 Query 有关的表 DDL 信息，然后将检索到的表 DDL 一起嵌入到预设的 Prompt 模板中，将 Prompt 和 Query 输入到 LLM 中，以生成最终的 SQL。</p>
<span id="more"></span>
<h6 id="整体的流程图如下（分为5个阶段）：">整体的流程图如下（分为5个阶段）：</h6>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250405143455628.png" alt=""></p>
<h6 id="第一步，元数据索引构建。">第一步，元数据索引构建。</h6>
<p>基于企业的业务库准备 AI 元数据，通过将数据库中的表结构、字段说明、约束关系等元数据经过 Vector Embedding 模型进行向量化处理，构建高效的语义检索体系，包括向量索引和 KV 索引。它们将被用来辅助后续的表召回过程。</p>
<h6 id="第二步，信息提取。">第二步，信息提取。</h6>
<p>通过语言大模型对输入 Query 进行信息提取，提取的内容主要包括 Query 句式（骨架）抽取、目标字段抽取、条件字段抽取、表名抽取。</p>
<h6 id="第三步，表和样例召回。">第三步，表和样例召回。</h6>
<p>将目标字段、条件字段、表名的抽取内容通过文本模型向量化后，基于第一步中准备的 AI 向量数据库进行向量相似性检索，结合表召回策略以及排序规则，得到召回的表名信息，以获得 Query 中涉及的所有表 DDL 信息。同时，将 Query 句式（骨架）抽取的内容通过文本模型向量化，与样例向量库数据集进行逐一相似度比对，结合样例召回策略，获取最终填充 Prompt 模版的样例。</p>
<h6 id="第四步，Prompt-动态组装。">第四步，Prompt 动态组装。</h6>
<p>将步骤三中得到的表 DDL 信息、样例信息按照模版进行 Prompt 生成组装。</p>
<h6 id="第五步，SQL-生成与输出。">第五步，SQL 生成与输出。</h6>
<p>将第四步中得到的 Prompt 和原始的 Query 一起输入大模型，并进行 SQL 语句生成，将生成的 SQL 语句返回给数据库执行，完成一次问答的流程闭环。</p>
<h3 id="Text-to-SQL-项目介绍和对比">Text-to-SQL 项目介绍和对比</h3>
<p>以下是几个 <strong>Text-to-SQL</strong> 项目，涵盖开源方案和商业 API 集成方式：</p>
<h3 id="1-基于-OpenAI-GPT-的-Text2SQL-方案（支持其他类Openai接口）"><strong>1. 基于 OpenAI / GPT 的 Text2SQL 方案（支持其他类Openai接口）</strong></h3>
<ul>
<li>
<p><strong>LangChain SQLDatabaseChain</strong></p>
<ul>
<li>通过 <code>SQLDatabaseChain</code> 或 <code>SQLDatabaseSequentialChain</code> 直接调用 OpenAI 的 GPT 系列模型（如 <code>gpt-3.5-turbo</code> 或 <code>gpt-4</code>）生成 SQL。</li>
<li>支持动态 schema 解析，适用于 MySQL、PostgreSQL 等数据库。</li>
<li>示例代码：<pre><code class="language-python">from langchain.llms import OpenAI
from langchain_experimental.sql import SQLDatabaseChain
from langchain.utilities import SQLDatabase

db = SQLDatabase.from_uri(&quot;sqlite:///mydatabase.db&quot;)
llm = OpenAI(temperature=0)
db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)
result = db_chain.run(&quot;查询销售额最高的产品&quot;)
</code></pre>
</li>
<li><strong>优点</strong>：简单易用，适合快速对接 GPT API。</li>
<li><strong>缺点</strong>：依赖 OpenAI 接口，可能涉及数据隐私问题，效果取决于接口大模型的能力。</li>
</ul>
</li>
<li>
<p><strong>Vanna</strong>  （<a href="https://github.com/vanna-ai/vanna%EF%BC%89">https://github.com/vanna-ai/vanna）</a></p>
<ul>
<li>提供 Python 库，支持 OpenAI、Anthropic Claude 等大模型，可本地部署或云端调用。</li>
<li>支持 RAG（检索增强生成），结合向量数据库优化 SQL 生成。</li>
<li>示例代码：<pre><code class="language-python">import vanna as vn
vn.set_api_key(&quot;your_vanna_key&quot;)  # 申请 API Key
vn.connect_to_sqlite(&quot;mydatabase.db&quot;)
sql = vn.generate_sql(&quot;查询销售额最高的产品&quot;)
</code></pre>
</li>
<li><strong>优点</strong>：支持多模型切换，提供 Web UI 交互界面，开源可以本地部署。</li>
<li><strong>缺点</strong>：云端服务部分功能需付费。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-开源微调模型（可本地部署）"><strong>2. 开源微调模型（可本地部署）</strong></h3>
<ul>
<li>
<p><strong>SQLCoder（<a href="http://Defog.ai">Defog.ai</a>）</strong>  （<a href="https://github.com/defog-ai/sqlcoder%EF%BC%89">https://github.com/defog-ai/sqlcoder）</a></p>
<ul>
<li>基于 StarCoder 微调，性能接近 GPT-4，支持 PostgreSQL、MySQL 等。</li>
<li>可本地运行，避免数据外传。</li>
<li>示例代码：<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;defog/sqlcoder&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;defog/sqlcoder&quot;)
inputs = tokenizer(&quot;查询销售额最高的产品&quot;, return_tensors=&quot;pt&quot;)
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))
</code></pre>
</li>
<li><strong>优点</strong>：开源免费，性能强。</li>
<li><strong>缺点</strong>：需较高 GPU 资源（如 A100）。</li>
</ul>
</li>
<li>
<p><strong>DB-GPT-Hub</strong>  （<a href="https://github.com/eosphoros-ai/DB-GPT-Hub%EF%BC%89">https://github.com/eosphoros-ai/DB-GPT-Hub）</a></p>
<ul>
<li>支持 LLaMA-2、CodeLlama 等模型微调，适配 Spider、BIRD 等数据集。</li>
<li>提供完整训练-推理流程，适合企业定制化需求。</li>
<li><strong>优点</strong>：可私有化部署，支持 LoRA 低资源微调。</li>
<li><strong>缺点</strong>：需自行训练模型。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-商业-API-方案"><strong>3. 商业 API 方案</strong></h3>
<ul>
<li><strong>Google Cloud’s Text-to-SQL</strong>（实验性）
<ul>
<li>集成 BigQuery，支持自然语言转 SQL。</li>
</ul>
</li>
<li><strong>Microsoft Semantic Kernel + Azure OpenAI</strong>
<ul>
<li>结合 Azure 数据库服务，提供企业级 Text2SQL 方案。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="对比总结"><strong>对比总结</strong></h3>
<table>
<thead>
<tr>
<th>方案</th>
<th>开源</th>
<th>依赖大模型 API</th>
<th>本地部署</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LangChain + OpenAI</strong></td>
<td>❌/✅</td>
<td>✅（GPT-4等）</td>
<td>❌/✅</td>
<td>快速原型开发</td>
</tr>
<tr>
<td><strong>Vanna</strong></td>
<td>✅（部分功能付费）</td>
<td>✅（多模型）</td>
<td>✅</td>
<td>企业级 RAG 方案</td>
</tr>
<tr>
<td><strong>SQLCoder</strong></td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>高性能本地化需求</td>
</tr>
<tr>
<td><strong>DB-GPT-Hub</strong></td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>定制化微调</td>
</tr>
</tbody>
</table>
<p><strong>推荐选择</strong>：</p>
<ul>
<li><strong>快速尝试</strong>：用 LangChain + OpenAI 或 Vanna。</li>
<li><strong>数据安全优先</strong>：用 SQLCoder 或 DB-GPT-Hub 本地部署。</li>
<li><strong>企业级应用</strong>：结合 RAG（如 Vanna）或微调（如 DB-GPT-Hub）。</li>
</ul>
<hr>
<h3 id="附上更全的主流技术方案对比：">附上更全的主流技术方案对比：</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Chat2db</strong></td>
<td>人工智能驱动的数据管理平台，支持多种数据库</td>
<td>支持多种数据库，提供7B开源模型</td>
<td>需要集成多种数据库，可能存在兼容性问题</td>
</tr>
<tr>
<td><strong>SQL Chat</strong></td>
<td>基于聊天的SQL客户端，使用自然语言与数据库通信</td>
<td>支持多种数据库系统，用户友好</td>
<td>可能需要额外的配置来适应特定的数据库</td>
</tr>
<tr>
<td><strong>Vanna</strong></td>
<td>开源Python RAG框架，整合上下文和领域知识文档训练模型</td>
<td>支持自定义可视化UI，灵活度高</td>
<td>需要专业知识来训练和维护模型</td>
</tr>
<tr>
<td><strong>Dataherald</strong></td>
<td>自然语言到SQL引擎，用于企业级问答</td>
<td>模块化设计，易于扩展和维护</td>
<td>需要业务用户适应自然语言到SQL的转换</td>
</tr>
<tr>
<td><strong>WrenAI</strong></td>
<td>文本到SQL解决方案，无需编写SQL即可查询数据</td>
<td>易于使用，安全可靠，高度准确</td>
<td>需要用户适应自然语言查询的方式</td>
</tr>
<tr>
<td><strong>SuperSonic</strong></td>
<td>腾讯音乐开发的模型知识库和语义解析器</td>
<td>强大的语义解析能力，支持多种数据库</td>
<td>可能需要专业的知识来理解和使用</td>
</tr>
<tr>
<td><strong>Awesome Text 2SQL</strong></td>
<td>精选教程资源库，包含LMs、Text2SQL等方面的模型</td>
<td>提供丰富的学习资源和模型</td>
<td>主要作为资源库，可能需要额外的开发工作来集成实际应用中</td>
</tr>
<tr>
<td><strong>DuckDB-NSQL</strong></td>
<td>为DuckDB SQL分析任务构建的Text 2SQL LL M</td>
<td>帮助用户利用DuckDB的全部功能</td>
<td>特定于DuckDB，可能不适用于其他数据库系统</td>
</tr>
<tr>
<td><strong>Langchain</strong></td>
<td>在SQL数据库上构建问答链代理的应用框架</td>
<td>支持构建问答链代理，运行生成的查询并从错误中恢复</td>
<td>需要一定的技术背景来构建和维护问答系统</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
        <category>Text2SQL</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>AI</tag>
        <tag>Text2SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2025/05/30/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>markdown-it 的扩展插件</title>
    <url>/2025/03/09/markdown-it-%E7%9A%84%E6%89%A9%E5%B1%95%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>这些插件是 <code>markdown-it</code> 的扩展插件，用于增强 Markdown 的功能。以下是每个插件的功能说明和使用方法：</p>
<hr>
<h3 id="1-markdown-it-footnote">1. <strong>markdown-it-footnote</strong></h3>
<ul>
<li>
<p><strong>功能</strong>：支持脚注功能。</p>
</li>
<li>
<p><strong>用法</strong>：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">这是一个带有脚注的文本[^footnote]。</span><br><span class="line"></span><br><span class="line">[<span class="symbol">^footnote</span>]: <span class="link">这是脚注内容。</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>效果</strong>：<br>
这是一个带有脚注的文本<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
</li>
</ul>
<hr>
<span id="more"></span>
<h3 id="2-markdown-it-sub">2. <strong>markdown-it-sub</strong></h3>
<ul>
<li><strong>功能</strong>：支持下标文本。</li>
<li><strong>用法</strong>：<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">H~2~O</span><br></pre></td></tr></table></figure>
</li>
<li><strong>效果</strong>：<br>
H<sub>2</sub>O</li>
</ul>
<hr>
<h3 id="3-markdown-it-sup">3. <strong>markdown-it-sup</strong></h3>
<ul>
<li><strong>功能</strong>：支持上标文本。</li>
<li><strong>用法</strong>：<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">2^10^ = 1024</span><br></pre></td></tr></table></figure>
</li>
<li><strong>效果</strong>：<br>
2<sup>10</sup> = 1024</li>
</ul>
<hr>
<h3 id="4-markdown-it-deflist">4. <strong>markdown-it-deflist</strong></h3>
<ul>
<li>
<p><strong>功能</strong>：支持定义列表。</p>
</li>
<li>
<p><strong>用法</strong>：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">术语 1</span><br><span class="line">: 定义 1</span><br><span class="line"></span><br><span class="line">术语 2</span><br><span class="line">: 定义 2</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>效果</strong>：<br>
术语 1<br>
: 定义 1</p>
<dl>
<dt>术语 2</dt>
<dd>定义 2</dd>
</dl>
</li>
</ul>
<hr>
<h3 id="5-markdown-it-abbr">5. <strong>markdown-it-abbr</strong></h3>
<ul>
<li>
<p><strong>功能</strong>：支持缩写词。</p>
</li>
<li>
<p><strong>用法</strong>：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="emphasis">*[HTML]: HyperText Markup Language</span></span><br><span class="line"><span class="emphasis">*</span>[W3C]: World Wide Web Consortium</span><br><span class="line"></span><br><span class="line">HTML 是 W3C 的标准。</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>效果</strong>：</p>
<p><abbr title="HyperText Markup Language">HTML</abbr> 是 <abbr title="World Wide Web Consortium">W3C</abbr> 的标准。</p>
</li>
</ul>
<hr>
<h3 id="6-markdown-it-emoji">6. <strong>markdown-it-emoji</strong></h3>
<ul>
<li><strong>功能</strong>：支持 Emoji 表情。</li>
<li><strong>用法</strong>：<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">:smile: :heart: :rocket:</span><br></pre></td></tr></table></figure>
</li>
<li><strong>效果</strong>：<br>
😄 ❤️ 🚀</li>
</ul>
<hr>
<h3 id="7-markdown-it-container">7. <strong>markdown-it-container</strong></h3>
<ul>
<li>
<p><strong>功能</strong>：支持自定义容器块。</p>
</li>
<li>
<p><strong>用法</strong>：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">::: warning</span><br><span class="line">这是一个警告框。</span><br><span class="line">:::</span><br><span class="line"></span><br><span class="line">::: tip</span><br><span class="line">这是一个提示框。</span><br><span class="line">:::</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>效果</strong>：<br>
::: warning<br>
这是一个警告框。<br>
:::</p>
<p>::: tip<br>
这是一个提示框。<br>
:::</p>
</li>
</ul>
<hr>
<h3 id="8-markdown-it-ins">8. <strong>markdown-it-ins</strong></h3>
<ul>
<li><strong>功能</strong>：支持插入文本（下划线）。</li>
<li><strong>用法</strong>：<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">++插入的文本++</span><br></pre></td></tr></table></figure>
</li>
<li><strong>效果</strong>：<br>
<ins>插入的文本</ins></li>
</ul>
<hr>
<h3 id="9-markdown-it-mark">9. <strong>markdown-it-mark</strong></h3>
<ul>
<li><strong>功能</strong>：支持高亮文本。</li>
<li><strong>用法</strong>：<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">==高亮的文本==</span><br></pre></td></tr></table></figure>
</li>
<li><strong>效果</strong>：<br>
<mark>高亮的文本</mark></li>
</ul>
<hr>
<h3 id="10-配置方法">10. <strong>配置方法</strong></h3>
<p>在 <code>_config.yml</code> 中配置 <code>hexo-renderer-markdown-it</code> 插件时，可以启用这些插件。例如：</p>
   <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">markdown:</span></span><br><span class="line">  <span class="attr">render:</span></span><br><span class="line">    <span class="attr">html:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">xhtmlOut:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">breaks:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">linkify:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">typographer:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">quotes:</span> <span class="string">&#x27;“”‘’&#x27;</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-footnote</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-sub</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-sup</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-deflist</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-abbr</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-emoji</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-container</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-ins</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-mark</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="11-安装插件">11. <strong>安装插件</strong></h3>
<p>运行以下命令安装这些插件：</p>
   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pnpm install markdown-it-footnote markdown-it-sub markdown-it-sup markdown-it-deflist markdown-it-abbr markdown-it-emoji markdown-it-container markdown-it-ins markdown-it-mark --save</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="12-总结">12. <strong>总结</strong></h3>
<ul>
<li>这些插件可以增强 Markdown 的功能，例如支持脚注、上下标、定义列表、Emoji 表情等。</li>
<li>在 <code>_config.yml</code> 中配置 <code>hexo-renderer-markdown-it</code> 插件时，启用这些插件即可使用。</li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>这是脚注内容。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>自媒体</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Hexo</tag>
        <tag>Markdown插件</tag>
        <tag>自媒体</tag>
      </tags>
  </entry>
  <entry>
    <title>rag-lesson-1</title>
    <url>/2025/03/05/rag-lesson-1/</url>
    <content><![CDATA[<h1>从 0 到 1 快速搭建 RAG 应用</h1>
<ul>
<li>
<h2 id="技术框架与选型">技术框架与选型</h2>
<ul>
<li>
<h3 id="RAG-技术框架：LangChain">RAG 技术框架：<strong>LangChain</strong></h3>
<ul>
<li>LangChain 是专为开发基于大型语言模型（LLM）应用而设计的全面框架，其核心目标是简化开发者的构建流程，使其能够高效创建 LLM 驱动的应用。</li>
</ul>
</li>
</ul>
<span id="more"></span>
<ul>
<li>
<h3 id="索引流程-文档解析模块：pypdf">索引流程 - 文档解析模块：pypdf</h3>
<ul>
<li>pypdf 是一个开源的 Python 库，专门用于处理 PDF 文档。pypdf 支持 PDF 文档的创建、读取、编辑和转换操作，能够有效提取和处理文本、图像及页面内容。</li>
</ul>
</li>
<li>
<h3 id="索引流程-文档分块模块：RecursiveCharacterTextSplitter">索引流程 - 文档分块模块：RecursiveCharacterTextSplitter</h3>
<ul>
<li>采用 LangChain 默认的文本分割器 -RecursiveCharacterTextSplitter。该分割器通过层次化的分隔符（从双换行符到单字符）拆分文本，旨在保持文本的结构和连贯性，优先考虑自然边界如段落和句子。</li>
</ul>
</li>
<li>
<h3 id="索引-检索流程-向量化模型：BAAI-bge-m3">索引 / 检索流程 - 向量化模型：BAAI/bge-m3</h3>
<ul>
<li>bge-m3 是由北京人工智能研究院（BAAI，智源）开发的开源向量模型。能够提供高精度和高效的中文向量检索。该模型的向量维度为 1024，最大输入长度同样为 8k。</li>
</ul>
</li>
<li>
<h3 id="索引-检索流程-向量库：Faiss">索引 / 检索流程 - 向量库：Faiss</h3>
<ul>
<li>Faiss 全称 Facebook AI Similarity Search，由 Facebook AI Research 团队开源的向量库，因其稳定性和高效性在向量检索领域广受欢迎。</li>
</ul>
</li>
<li>
<h3 id="生成流程-大语言模型：SiliconCloud-硅基流动-的-Qwen-Qwen2-5-7B-Instruct">生成流程 - 大语言模型：SiliconCloud(硅基流动)的 Qwen/Qwen2.5-7B-Instruct</h3>
<ul>
<li>Qwen/Qwen2.5-7B-Instruct 是阿里开源的一款 chat 大语言模型，支持对话、文案创作、逻辑推理、以及多语言处理，在模型性能和工程应用中表现出色。</li>
</ul>
</li>
<li>
<h3 id="上述选型在-RAG-流程图中的应用如下所示：">上述选型在 RAG 流程图中的应用如下所示：</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/RAG%E6%B5%81%E7%A8%8B.png" alt="RAG流程"></p>
<ul>
<li>LangChain：提供用于构建 LLM RAG 的应用程序框架。</li>
<li>索引流程：使用 pypdf 对文档进行解析并提取信息；随后，采用 RecursiveCharacterTextSplitter 对文档内容进行分块（chunks）；最后，利用 bge-3m 将分块内容进行向量化处理，并将生成的向量存储在 Faiss 向量库中。</li>
<li>检索流程：使用 bge-3m 对用户的查询（Query）进行向量化处理；然后，通过 Faiss 向量库对查询向量和文本块向量进行相似度匹配，从而检索出与用户查询最相似的前 top-k 个文本块（chunk）。</li>
<li>生成流程：通过设定提示模板（Prompt），将用户的查询与检索到的参考文本块组合输入到 Qwen 大模型中，生成最终的 RAG 回答。</li>
</ul>
</li>
</ul>
</li>
<li>
<h2 id="开发环境与技术库">开发环境与技术库</h2>
<ul>
<li>
<h3 id="准备windows系统，-pycharm开发工具，-Miniconda（python环境管理工具）">准备windows系统， pycharm开发工具， Miniconda（python环境管理工具）</h3>
</li>
<li>
<h3 id="创建并激活虚拟环境">创建并激活虚拟环境</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n rag python=<span class="number">3.11</span>  <span class="comment"># 创建名为rag的虚拟环境</span></span><br><span class="line">conda activate rag  <span class="comment"># 激活虚拟环境</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<h3 id="安装技术依赖库">安装技术依赖库</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install langchain langchain_community pypdf sentence-transformers faiss-cpu</span><br><span class="line"><span class="comment"># 无法安装，可以使用国内镜像源，命令如下：</span></span><br><span class="line">pip install langchain langchain_community pypdf sentence-transformers faiss-cpu -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
</li>
<li>
<h3 id="课程中用到的代码和pdf文件分享在github上：">课程中用到的代码和pdf文件分享在github上：</h3>
<ol>
<li>[x] <a href="https://github.com/hfhfn/rag_learning">https://github.com/hfhfn/rag_learning</a></li>
</ol>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>AI,</tag>
      </tags>
  </entry>
  <entry>
    <title>windows下使用certify申请SSL证书配置https访问</title>
    <url>/2025/04/25/windows%E4%B8%8B%E4%BD%BF%E7%94%A8certify%E7%94%B3%E8%AF%B7SSL%E8%AF%81%E4%B9%A6%E9%85%8D%E7%BD%AEhttps%E8%AE%BF%E9%97%AE/</url>
    <content><![CDATA[<h5 id="certbot-已停止windows维护（虽然目前官方仍在更新），推荐使用-Certify-the-Web-Certify-工具来管理-SSL-证书。以下是-Certify-的详细使用指南，适用于-Windows-服务器（如果是-Linux，建议仍用-certbot-或-acme-sh）。"><code>certbot</code> 已停止windows维护（虽然目前官方仍在更新），推荐使用 **Certify the Web (Certify)**工具来管理 SSL 证书。以下是 <strong>Certify 的详细使用指南</strong>，适用于 Windows 服务器（如果是 Linux，建议仍用 <code>certbot</code> 或 <code>acme.sh</code>）。</h5>
<span id="more"></span>
<hr>
<h2 id="1-Certify-the-Web-简介"><strong>1. Certify the Web 简介</strong></h2>
<ul>
<li><strong>适用平台</strong>：Windows（IIS / 其他 Web 服务器）</li>
<li><strong>功能</strong>：
<ul>
<li>自动申请 <strong>Let’s Encrypt</strong> 证书（免费）。</li>
<li>支持 <strong>通配符证书</strong>（Wildcard SSL）。</li>
<li>自动续期（无需手动操作）。</li>
<li>支持 DNS API 验证（如阿里云DNS、Cloudflare等）。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-安装-Certify-the-Web"><strong>2. 安装 Certify the Web</strong></h2>
<h3 id="下载与安装"><strong>下载与安装</strong></h3>
<ol>
<li><strong>访问官网</strong>：<a href="https://certifytheweb.com/">Certify the Web</a></li>
<li>下载并安装 <strong>Certify The Web</strong>（支持 Windows Server 2012+）。</li>
</ol>
<hr>
<h2 id="3-配置-Certify"><strong>3. 配置 Certify</strong></h2>
<h5 id="新建证书">新建证书</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425022603557.png" alt="image-20250425022603557"></p>
<h5 id="配置验证方式">配置验证方式</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425023900503.png" alt="image-20250425023900503"></p>
<h5 id="申请DNS-Access-Key">申请DNS Access Key</h5>
<ul>
<li>登录 <a href="https://ram.console.aliyun.com/users">阿里云RAM控制台</a>。</li>
<li>创建 <strong>用户</strong>，赋予 <code>AliyunDNSFullAccess</code> 权限。</li>
<li>在 Certify 中配置 <strong>AccessKey ID</strong> 和 <strong>AccessKey Secret</strong>。</li>
<li>最后在Certify界面，点击 <strong>Request Certificate</strong> ，就能自动完成申请证书了</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425030239523.png" alt=""></p>
<hr>
<h2 id="4-自动续期"><strong>4. 自动续期</strong></h2>
<ul>
<li>Certify 默认会 <strong>自动续期</strong>（证书到期前 30 天）。</li>
</ul>
<hr>
<h2 id="5-在-Nginx-中使用证书"><strong>5. 在 Nginx 中使用证书</strong></h2>
<p>如果后端是 <strong>Nginx</strong>（非IIS），需导出证书：</p>
<ol>
<li>
<p><strong>导出证书</strong>：</p>
<ul>
<li>按照下图操作完证书就保存到本地目录了：
<ul>
<li><code>cert.pem</code>（证书）</li>
<li><code>privkey.pem</code>（私钥）<br>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425033111550.png" alt="image-20250425033111550"></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>配置 Nginx</strong>：简单示例，配置好重启nginx，就可以使用域名访问nginx转发的端口了</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">server_name</span> 域名;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">ssl_certificate</span> /path/to/cert.pem;</span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /path/to/privkey.pem;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 其他配置...</span></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://localhost:3000;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h2 id="6-如果本地启用了clash代理，可以配置域名绕过代理"><strong>6. 如果本地启用了clash代理，可以配置域名绕过代理</strong></h2>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425034209450.png" alt="image-20250425034209450"></p>
]]></content>
      <categories>
        <category>工具</category>
        <category>网络服务</category>
      </categories>
      <tags>
        <tag>申请certify证书</tag>
        <tag>SSL证书</tag>
        <tag>配置https访问</tag>
        <tag>clash配置域名绕过代理</tag>
      </tags>
  </entry>
  <entry>
    <title>zerotier内网穿透</title>
    <url>/2025/04/25/zerotier%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</url>
    <content><![CDATA[<h4 id="zerotier一个操作简单的基于p2p的内网穿透方案：地址：https-my-zerotier-com">zerotier一个操作简单的基于p2p的内网穿透方案：地址：<a href="https://my.zerotier.com/">https://my.zerotier.com/</a></h4>
<span id="more"></span>
<h5 id="注册账号，然后创建网络">注册账号，然后创建网络</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425010852000.png" alt="image-20250425010852000"></p>
<h5 id="配置流程">配置流程</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425011141517.png" alt="image-20250425011141517"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425012257861.png" alt="image-20250425012257861"></p>
<h6 id="下载页面">下载页面</h6>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425011422200.png" alt="image-20250425011422200"></p>
<h6 id="安装好程序后进行配置">安装好程序后进行配置</h6>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425012021676.png" alt="image-20250425012021676"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425012825093.png" alt="image-20250425012825093"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425013650003.png" alt="image-20250425013650003"></p>
<h6 id="然后使用下图中的IP就可以访问本地端口了">然后使用下图中的IP就可以访问本地端口了</h6>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425014142640.png" alt="image-20250425014142640"></p>
]]></content>
      <categories>
        <category>工具</category>
        <category>网络服务</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>内网穿透</tag>
      </tags>
  </entry>
  <entry>
    <title>一文看懂RAG的前世今生</title>
    <url>/2025/04/01/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82RAG%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
    <content><![CDATA[<h2 id="1-大语言模型（LLM）的局限性">1. 大语言模型（LLM）的局限性</h2>
<ol>
<li>LLM 容易出现“幻觉”现象，即<strong>生成不准确或虚构的内容</strong>。</li>
<li>LLM 的<strong>上下文窗口有限</strong>，无法处理过长的文本。</li>
<li>LLM 存在隐私泄露的风险，也就是<strong>数据安全</strong>问题。</li>
</ol>
<p>为了弥补这些不足 RAG（检索增强生成）技术应运而生，也就是最早的<strong>Naive（朴素的） RAG</strong>或者说<strong>Vanilla（普通的） RAG</strong></p>
<p>使用如<strong>BM25（TF-IDF）或者Dense（Embedding） Retrieval</strong>的方式。</p>
<h2 id="2-RAG技术在适应复杂应用场景和不断发展的技术需求中经历了从Naive-RAG-，到流程优化的-Advanced-RAG，再到更具灵活性的-Modular-RAG-的演变。">2. RAG技术在适应复杂应用场景和不断发展的技术需求中经历了从Naive RAG ，到流程优化的 Advanced RAG，再到更具灵活性的 Modular RAG 的演变。</h2>
<span id="more"></span>
<img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250401131315866.png" alt="image-20250401131315866" style="zoom: 80%;" />
<h3 id="Advanced-RAG">Advanced RAG</h3>
<h5 id="检索前优化">检索前优化</h5>
<p>检索前优化通过索引、分块、查询优化以及内容向量化等技术手段，提高检索内容的精确性和生成内容的相关性。</p>
<ol>
<li><strong>滑动窗口方法</strong>：经典的分块技术，通过在相邻的文本块之间创建重叠区域，确保关键信息不会因简单的分段而丢失。这种方法在索引过程中通过在块之间保留重复部分，保证了检索时上下文信息的连贯性，进而提高了检索的精度。</li>
<li><strong>元数据添加</strong>：为每个分块添加元数据（如创建日期、章节名称、文档类型等），能够使系统在检索时快速过滤掉无关内容。例如，用户在查询时可以通过元数据筛选特定时间段的文档，减少不相关信息的干扰。</li>
<li>分层索引：在索引过程中，可以采用句子级、段落级甚至文档级的多层次嵌入方法。这样，系统可以根据查询的具体要求，灵活地在不同层次进行检索。比如，当用户输入较为复杂的长句查询时，段落级别的嵌入能够提供更加全面的语义匹配；而对于简短查询，句子级的嵌入能够提供更精确的结果。</li>
<li><strong>句子窗口检索</strong>：这种方法通过将文档中的每个句子独立嵌入，从而提高检索的精确度。在检索过程中，系统找到与查询最相关的句子后，会扩展句子前后的上下文窗口，保证生成模型能够获取足够的背景信息进行推理。这种方式既能够精准定位关键信息，又能确保生成的上下文连贯性。</li>
<li><strong>查询重写</strong>：针对用户输入的原始查询进行重新表述，使其更加清晰易懂，并且与检索任务匹配。例如，针对用户模糊或含糊的提问，系统可以通过重写，使查询更加具体化，从而检索到更加精准的内容。</li>
<li><strong>查询扩展</strong>：查询扩展通过增加同义词、相关词汇或概念扩展用户的原始查询，增加了检索结果的广度。这样，当用户输入简短或不完整的查询时，系统能够通过扩展词汇找到更多潜在相关的内容，从而提升检索效果。</li>
<li><strong>长短不一的内容向量化</strong>：RAG 系统中，文档或查询的长度对向量化过程有着显著的影响。对于短句子或短语，其生成的向量更加聚焦于具体细节，能够实现更精确的句子级别匹配。段落或文档级别的向量化涵盖了更广泛的上下文信息，能够捕捉到内容的整体语义。</li>
</ol>
<h5 id="检索优化">检索优化</h5>
<p>检索优化是 RAG 系统中直接影响检索效果和质量的核心环节。通过增强向量搜索、动态嵌入模型、混合检索等技术手段，系统能够高效、精准地找到与用户查询最相关的内容。</p>
<ol>
<li><strong>动态嵌入</strong>：RAG 系统通过动态嵌入模型根据上下文变化实时调整单词的嵌入表示，能够捕捉单词在不同上下文中的不同含义。例如，“bank”在“river bank”（河岸）和“financial bank”（银行）中的语义完全不同，动态嵌入可以根据具体语境生成合适的向量，从而提高检索的精准性。</li>
<li><strong>领域特定嵌入微调</strong>：在实际应用中，不同领域的数据语境差异较大，通用的嵌入模型往往无法覆盖某些领域的专业术语或特定语义。通过对嵌入模型进行微调，可以增强其在特定领域中的表现。例如，针对医学、法律等专业领域，可以对嵌入模型进行定制化训练 / 微调，使其更好地理解这些领域中的特有词汇和语境。</li>
<li><strong>假设文档嵌入</strong>：假设文档嵌入（Hypothetical Document Embeddings，HyDE）是一种创新的检索技术。HyDE 方法通过生成假设文档并将其向量化，以提升查询与检索结果之间的语义匹配度。当用户输入一个查询时，LLM 首先基于查询生成一个假设性答案，这个答案不一定是真实存在的文档内容，但它反映了查询的核心语义。然后，系统将该假设性答案向量化，与数据库中的向量进行匹配，寻找最接近的文档。例如，用户询问“拔除智齿需要多长时间？”，系统会生成一个假设性回答“拔智齿通常需要 30 分钟到两小时”，然后根据该假设文档进行检索，系统可能最终找到类似的真实文档，如“拔智齿的过程通常持续几分钟到 20 分钟以上”。通过假设文档，系统可以捕捉到更准确的相关文档。</li>
<li><strong>混合检索</strong>：混合检索是结合向量搜索与关键词搜索等多种检索方法的混合方法，能够同时利用语义匹配与关键词匹配的优势。</li>
<li><strong>小到大检索</strong>：这种方法首先通过较小的内容块（如单个句子或短段落）进行嵌入和检索，确保模型能找到与查询最匹配的小范围上下文。检索到相关内容后，再在生成阶段使用对应的较大文本块（如完整段落或全文）为模型提供更广泛的上下文支持。小块检索有助于提高精度，而大块生成则提供丰富的背景信息，使得生成的内容更加全面。</li>
<li><strong>递归块合并</strong>：通过逐级扩展检索内容，确保生成阶段能够捕捉到更全面的上下文信息。该技术在细粒度的子块检索后，自动将相关的父块合并，以便提供完整的上下文供生成模型参考。</li>
</ol>
<h5 id="检索后优化">检索后优化</h5>
<p>检索后优化目的是对已经检索到的内容进行进一步的处理和筛选，常用的技术包括重排序、提示压缩等，以确保最终生成的答案具有高度的相关性和准确性。</p>
<ol>
<li><strong>重排序</strong>：在 RAG 系统中，虽然初始检索可以找到多个与查询相关的内容块，但这些内容的相关性可能存在差异，因此需要进一步排序以优化生成结果。重排序通过重排序模型根据上下文的重要性、相关性评分等因素对已检索内容重新打分，以确保最相关的信息被优先处理。</li>
<li><strong>提示压缩</strong>：通过删除冗余信息、合并相关内容、突出关键信息等方式来压缩提示，为生成模型提供更简洁、更相关的输入。</li>
<li>上下文重构：通过对检索到的内容进行再加工或重组，以便更好地符合查询的需求。常见的做法是将多个检索到的上下文片段整合成一个更具连贯性的文本块，减少重复或冲突的内容，从而为生成模型提供一个统一、清晰的输入。</li>
<li><strong>内容过滤</strong>：根据预先设定的规则进行，包括过滤掉与查询无直接关联的内容、语义相似度较低的片段、冗长且无关的背景信息等，避免对生成结果产生负面影响。</li>
<li><strong>多跳推理</strong>：系统通过多个推理步骤，逐步整合信息，以回答复杂查询。通常用于需要跨多个上下文或多步推理的问题。例如，用户询问“某个技术的演化历程”，系统可能先检索到该技术的某个时间点的关键事件，然后再通过进一步检索，找到关于这些关键事件的详细说明，最终给出完整的回答。</li>
<li><strong>知识注入</strong>：在检索后通过外部知识库或预定义的领域知识，增强生成的上下文内容。这种方式适用于对准确性要求较高的场景，尤其是在特定领域或技术场景下，系统需要补充额外的专业知识。</li>
</ol>
<h3 id="Modular-RAG">Modular RAG</h3>
<p><strong>编排模块（Orchestration）</strong> 是 Modular RAG 区别于 Advanced RAG 的核心，它通过灵活的路由、调度、知识引导与推理路径来动态决定处理流程，从而提升了整个系统在复杂查询场景下的适应性和处理能力。</p>
<ol>
<li>
<p><strong>Routing（路由）</strong></p>
<p>路由是编排流程中的关键步骤。它的主要功能是在收到用户查询后，根据查询的特点和上下文，选择最合适的流程。具体来说，Routing 模块依赖于以下两部分：</p>
<p><strong>a. Query Analysis（查询分析）</strong>：首先，对用户的查询进行语义分析，判断其类型和难度。例如，一个直接问答式的查询可能不需要复杂的检索过程，而一个涉及多步推理的复杂问题则可能需要走更长的检索路径。</p>
<p><strong>b. Pipeline Selection（管道选择）</strong>：根据查询分析的结果，Routing 模块会动态选择合适的流程（Pipeline）。比如针对简单的查询，可以仅用大模型的知识来回答，效率高。而针对需要领域知识及复杂推理的查询，系统会使用更多的检索步骤，结合外部文档及知识进行深度检索生成。</p>
</li>
<li>
<p><strong>Scheduling（调度）</strong></p>
<p>调度的作用是管理查询的执行顺序，并动态调整检索和生成步骤。</p>
<p><strong>a. Query Scheduling（查询调度）</strong>：当系统接收到查询时，调度模块会判断是否需要进行检索。调度模块根据查询的重要性、上下文信息、已有生成结果的质量等多维度因素进行评估。</p>
<p><strong>b. Judgment of Retrieval Needs（检索需求判断）</strong>：调度还通过特定的判断节点来确定是否需要额外检索。在某些情况下，系统可能会多次判断是否有必要执行新一轮的检索。</p>
</li>
<li>
<p><strong>Knowledge Guide（知识引导）</strong></p>
<p>知识引导是结合知识图谱和推理路径来增强查询处理过程。</p>
<p><strong>a. Knowledge Graph（知识图谱）</strong>：在处理复杂查询时，系统可以调用知识图谱来辅助检索。这不仅提升了检索结果的准确性，还可以通过知识图谱中的上下文关系来推导出更为精确的答案。例如，若查询涉及多个实体的关系或多个时间点，知识图谱能够提供更深层次的推理支持。</p>
<p><strong>b. Reasoning Path（推理路径）</strong>：通过推理路径，系统可以设计出一条符合查询需求的推理链条，系统可以根据这一链条进行逐步地推理和检索。这在处理具有强逻辑性的问题时非常有效，例如跨多个文档的关系推理或时间序列推导。</p>
</li>
</ol>
<p>Modular RAG 进一步增强了 RAG 系统的灵活性和适应性，允许通过不同模块的自由组合来应对多样化的应用场景，实现了更加智能的流程控制和动态决策。</p>
<h2 id="3-GraphRAG解决向量检索面临的两个主要挑战（无法捕捉人类长期记忆的两个关键特征）">3. GraphRAG解决向量检索面临的两个主要挑战（无法捕捉人类长期记忆的两个关键特征）</h2>
<p>• <strong>意义构建（sense-making）：即归纳总结，指理解大规模或复杂事件的能力</strong>，标准的RAG方法在处理需要理解大规模或复杂事件、经历或数据的能力（即情境理解）方面存在不足。<strong>这种能力对于理解和推理长篇故事或复杂文本至关重要</strong>。</p>
<p>• <strong>关联性（associativity）：关联性则是指在不同知识片段之间建立多跳连接的能力</strong>，难以在多跳问答任务中表现出色，因为它们依赖于独立的向量检索，<strong>无法有效地在多个信息片段之间建立多跳连接</strong>。这对于需要从不同来源或段落中提取信息来回答问题是关键的限制。</p>
<p>为了解决这些挑战，<strong>微软提出了 GraphRAG</strong>，通过利用大模型生成的知识图谱来改进 RAG 的检索部分。GraphRAG 的核心创新在于<strong>利用结构化的实体和关系信息</strong>，使检索过程更加精准和全面，特别在处理多跳问题和复杂文档分析时表现突出。</p>
<h3 id="GraphRAG的原理：">GraphRAG的原理：</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/x1.png" alt="Refer to caption"></p>
<h3 id="传统RAG和GraphRAG对比：">传统RAG和GraphRAG对比：</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250401142025317.png" alt="image-20250401142025317"></p>
<h3 id="9种GraphRAG对比：（并附上各种方法已有的开源实现）">9种GraphRAG对比：（并附上各种方法已有的开源实现）</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250401145306424.png" alt="image-20250401145306424"></p>
<p>以下是表格中所有方法的简要解释，按它们在表格中出现的顺序排列：</p>
<hr>
<h4 id="1-ZeroShot（基于HuggingFace-Transformers实现）"><strong>1. ZeroShot</strong>（<a href="https://github.com/huggingface/transformers">基于HuggingFace Transformers实现</a>）</h4>
<ul>
<li><strong>定义</strong>：零样本方法（Zero-Shot）指模型在未经特定数据集训练的情况下直接进行预测或推理。</li>
<li><strong>特点</strong>：依赖预训练模型的泛化能力，无需微调或示例（prompt engineering）。</li>
<li><strong>适用场景</strong>：快速基线测试或资源受限的任务。</li>
</ul>
<hr>
<h4 id="2-VanillaRAG（基于langchain库实现-https-github-com-langchain-ai-langchain）"><strong>2. VanillaRAG</strong>（基于langchain库实现 <a href="https://github.com/langchain-ai/langchain%EF%BC%89">https://github.com/langchain-ai/langchain）</a></h4>
<ul>
<li><strong>定义</strong>：标准的检索增强生成（Retrieval-Augmented Generation）方法，结合检索模块（如BM25/Dense Retrieval）和生成模型（如LLM）。</li>
<li><strong>特点</strong>：通过检索相关文档辅助生成答案，但未针对复杂任务优化。</li>
<li><strong>适用场景</strong>：通用问答任务，性能中等。</li>
</ul>
<hr>
<h4 id="3-G-retriever（https-github-com-XiaoxinHe-G-Retriever）"><strong>3. G-retriever</strong>（<a href="https://github.com/XiaoxinHe/G-Retriever%EF%BC%89">https://github.com/XiaoxinHe/G-Retriever）</a></h4>
<ul>
<li><strong>定义</strong>：基于图的检索方法（Graph-based Retriever），可能利用知识图谱或文档间关系改进检索。</li>
<li><strong>特点</strong>：强调结构化检索，但在表格中表现较差（部分数据集结果缺失）。</li>
<li><strong>适用场景</strong>：需结构化信息的任务，但可能计算成本高。</li>
</ul>
<hr>
<h4 id="4-TGG"><strong>4. TGG</strong></h4>
<ul>
<li><strong>定义</strong>：<strong>Transition-based Graph Generation</strong>，一种基于状态转移的图生成方法。</li>
<li><strong>特点</strong>：通过动态构建图结构优化检索或生成，性能中等。</li>
<li><strong>适用场景</strong>：多跳问答或复杂推理任务。</li>
</ul>
<hr>
<h4 id="5-KGP"><strong>5. KGP</strong></h4>
<ul>
<li><strong>定义</strong>：<strong>Knowledge-Guided Prompting</strong>，利用外部知识库引导生成。</li>
<li><strong>特点</strong>：结合显式知识增强模型，在PopQA等数据集表现较好。</li>
<li><strong>适用场景</strong>：需要外部知识支持的问答。</li>
</ul>
<hr>
<h4 id="6-DALK"><strong>6. DALK</strong></h4>
<ul>
<li><strong>定义</strong>：<strong>Dynamic Adaptive Latent Knowledge</strong>，动态调整潜在知识表示的方法。</li>
<li><strong>特点</strong>：在MultihopQA和HotpotQA中表现突出（紫色标记），但其他任务不稳定。</li>
<li><strong>适用场景</strong>：动态适应复杂查询的任务。</li>
</ul>
<hr>
<h4 id="7-LLightRAG-GLightRAG-HLightRAG（https-github-com-HKUDS-LightRAG）"><strong>7. LLightRAG / GLightRAG / HLightRAG</strong>（<a href="https://github.com/HKUDS/LightRAG%EF%BC%89">https://github.com/HKUDS/LightRAG）</a></h4>
<ul>
<li><strong>定义</strong>：轻量级RAG变体，可能通过不同优化策略（如<strong>Local</strong>、<strong>Global</strong>或<strong>Hybrid</strong>检索）降低计算成本。</li>
<li><strong>特点</strong>：
<ul>
<li><strong>LLightRAG</strong>：局部优化，性能中等。</li>
<li><strong>GLightRAG</strong>：全局优化，但在PopQA表现较差。</li>
<li><strong>HLightRAG</strong>：混合策略，综合性能较好。</li>
</ul>
</li>
<li><strong>适用场景</strong>：资源受限环境下的平衡方案。</li>
</ul>
<hr>
<h4 id="8-FastGraphRAG（https-github-com-circlemind-ai-fast-graphrag）"><strong>8. FastGraphRAG</strong>（<a href="https://github.com/circlemind-ai/fast-graphrag%EF%BC%89">https://github.com/circlemind-ai/fast-graphrag）</a></h4>
<ul>
<li><strong>定义</strong>：快速图结构RAG，可能通过简化图构建或检索流程提升效率。</li>
<li><strong>特点</strong>：在多个数据集（如MultihopQA、ALCE）表现接近最佳（橙色标记）。</li>
<li><strong>适用场景</strong>：需速度和性能兼顾的任务。</li>
</ul>
<hr>
<h4 id="9-HippoRAG（https-github-com-OSU-NLP-Group-HippoRAG）"><strong>9. HippoRAG</strong>（<a href="https://github.com/OSU-NLP-Group/HippoRAG%EF%BC%89">https://github.com/OSU-NLP-Group/HippoRAG）</a></h4>
<ul>
<li><strong>定义</strong>：<strong>Hierarchical Iterative Path-based RAG</strong>，通过分层路径优化多跳推理，是一种受人类长期记忆启发的新型RAG框架，使 LLM 能够持续整合外部文档中的知识。RAG +知识图谱+个性化网页排名。</li>
<li><strong>特点</strong>：在Quality和MusiqueQA中表现最佳（紫色标记），但其他任务一般。</li>
<li><strong>适用场景</strong>：需要分层推理的复杂问答。</li>
</ul>
<hr>
<h4 id="10-LGraphRAG（https-github-com-microsoft-graphrag）"><strong>10. LGraphRAG</strong>（<a href="https://github.com/microsoft/graphrag%EF%BC%89">https://github.com/microsoft/graphrag）</a></h4>
<ul>
<li><strong>定义</strong>：<strong>Latent Graph RAG</strong>，隐式学习文档间关系构建图结构。</li>
<li><strong>特点</strong>：在MultihopQA的Accuracy和Recall上最优（紫色标记）。</li>
<li><strong>适用场景</strong>：隐式关系建模任务。</li>
</ul>
<hr>
<h4 id="11-RAPTOR（https-github-com-parthsarthi03-raptor）"><strong>11. RAPTOR</strong>（<a href="https://github.com/parthsarthi03/raptor%EF%BC%89">https://github.com/parthsarthi03/raptor）</a></h4>
<ul>
<li><strong>定义</strong>：<strong>Recursive Abstractive Processing for Tree-Organized Retrieval</strong>，递归处理树状检索结构的方法。</li>
<li><strong>特点</strong>：
<ul>
<li>在最大数据集上改用K-means（因Gaussian Mixture计算超时）。</li>
<li>在MusiqueQA、ALCE等多项任务中全面领先（紫色/橙色标记）。</li>
</ul>
</li>
<li><strong>适用场景</strong>：大规模、复杂数据集上的高性能需求。</li>
</ul>
<hr>
<h4 id="小结"><strong>小结</strong></h4>
<ul>
<li><strong>最佳方法</strong>：<strong>RAPTOR</strong>综合表现最强，<strong>DALK</strong>和<strong>LGraphRAG</strong>在特定任务领先。</li>
<li><strong>轻量级选择</strong>：FastGraphRAG或HLightRAG适合效率优先场景。</li>
<li><strong>知识依赖</strong>：KGP和HippoRAG在知识密集型任务中表现优异。</li>
</ul>
<h2 id="4-Agentic-RAG（详细内容可查看论文地址）-附上开源实现">4. Agentic RAG（<a href="https://arxiv.org/pdf/2501.09136">详细内容可查看论文地址</a>）(<a href="https://github.com/zilliztech/deep-searcher">附上开源实现</a>)</h2>
<p><strong>Agentic RAG</strong>（基于智能代理的检索增强生成）是传统 <strong>RAG（Retrieval-Augmented Generation）</strong> 技术的增强版本，它通过引入 <strong>AI Agent（人工智能代理）</strong> 的动态规划、自主决策和多步推理能力，使RAG系统能够更智能地处理复杂查询任务。</p>
<h3 id="Agentic-RAG-的核心特点"><strong>Agentic RAG 的核心特点</strong></h3>
<ol>
<li><strong>动态任务编排</strong>
<ul>
<li>根据用户查询的意图，自动选择不同的检索策略（如向量搜索、知识图谱查询、摘要索引等）714。</li>
<li>支持多步骤推理（Multi-hop Reasoning），例如分解复杂问题为多个子查询并逐步解答58。</li>
</ul>
</li>
<li><strong>查询优化与反馈机制</strong>
<ul>
<li>如果初始检索结果不理想，Agentic RAG 可以自动<strong>改写查询</strong>或调整检索策略214。</li>
<li>具备 <strong>反思（Self-Reflection）</strong> 能力，评估检索结果的质量并决定是否需要重新检索14。</li>
</ul>
</li>
<li><strong>多数据源与工具集成</strong>
<ul>
<li>可结合外部工具（如Web搜索、数据库查询、API调用）来增强回答的准确性8。</li>
<li>支持跨文档、跨模态（文本、图像、结构化数据）的信息整合5。</li>
</ul>
</li>
<li><strong>模块化架构</strong>
<ul>
<li>可采用<strong>单Agent</strong>（统一规划）或<strong>多Agent</strong>（分层协作）架构，例如：
<ul>
<li><strong>Tool Agent</strong>：负责特定知识库的检索（如向量索引、摘要引擎）。</li>
<li><strong>Top Agent</strong>：协调多个Tool Agent，规划全局任务7。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="AI代理RAG系统的基本实现">AI代理RAG系统的基本实现</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250401161858764.png" alt="image-20250401161858764"></p>
<h3 id="多代理RAG系统">多代理RAG系统</h3>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250401162540092.png" alt="image-20250401162540092"></p>
<h2 id="5-最终总结">5. 最终总结</h2>
<h3 id="RAG范式的对比分析"><strong>RAG范式的对比分析</strong></h3>
<table>
<thead>
<tr>
<th><strong>范式</strong></th>
<th><strong>核心特点</strong></th>
<th><strong>优势</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>基础RAG</strong></td>
<td>• 基于关键词的检索（如TF-IDF、BM25）</td>
<td>• 简单易实现<br>• 适合事实型查询</td>
</tr>
<tr>
<td><strong>高级RAG</strong></td>
<td>• 稠密检索模型（如DPR）<br>• 神经排序与重排序<br>• 多跳检索</td>
<td>• 检索精度高<br>• 上下文相关性更强</td>
</tr>
<tr>
<td><strong>模块化RAG</strong></td>
<td>• 混合检索（稀疏+稠密）<br>• 工具与API集成<br>• 可组合的领域专用流程</td>
<td>• 灵活性和可定制性高<br>• 适合多样化应用场景<br>• 可扩展性强</td>
</tr>
<tr>
<td><strong>图RAG</strong></td>
<td>• 基于图结构的集成<br>• 多跳推理<br>• 通过节点实现上下文增强</td>
<td>• 支持关系推理<br>• 减少幻觉生成<br>• 适合结构化数据任务</td>
</tr>
<tr>
<td><strong>智能体RAG</strong></td>
<td>• 自主智能体<br>• 动态决策<br>• 迭代优化与工作流调整</td>
<td>• 实时适应变化<br>• 支持多领域任务扩展<br>• 准确性高</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="关键术语说明"><strong>关键术语说明</strong></h3>
<ul>
<li><strong>多跳检索（Multi-hop Retrieval）</strong>：通过多次检索逐步获取答案，适合复杂问题。</li>
<li><strong>稠密检索（Dense Retrieval）</strong>：使用神经网络（如DPR）将查询和文档映射为向量进行相似度匹配。</li>
<li><strong>图结构（Graph-based）</strong>：用节点和边表示数据关系，增强推理能力（如知识图谱）。</li>
<li><strong>自主智能体（Autonomous Agents）</strong>：能独立规划、决策和优化的AI代理。</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>RAG</tag>
        <tag>发展</tag>
        <tag>分享规划</tag>
      </tags>
  </entry>
  <entry>
    <title>多智能体框架ChatDev与MetaGPT介绍</title>
    <url>/2025/03/21/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>ChatDev、MetaGPT 等多智能体框架是近年来在人工智能领域，特别是多智能体系统（Multi-Agent Systems, MAS）和生成式 AI 领域的重要进展。这些框架旨在通过多个智能体（Agents）的协作，完成复杂的任务或生成高质量的内容。以下是对这些框架的简要介绍和对比：</p>
<hr>
<span id="more"></span>
<h3 id="1-ChatDev"><strong>1. ChatDev</strong></h3>
<p>ChatDev 是一个基于多智能体协作的软件开发框架，专注于通过多个智能体的交互来自动化软件开发流程。它模拟了软件开发生命周期中的不同角色（如产品经理、开发人员、测试人员等），并通过智能体之间的对话和协作来完成软件开发任务。</p>
<h4 id="特点："><strong>特点：</strong></h4>
<ul>
<li><strong>多角色智能体</strong>：每个智能体扮演不同的角色，模拟真实的软件开发团队。</li>
<li><strong>对话驱动</strong>：智能体之间通过自然语言对话进行协作，生成代码、文档和测试用例。</li>
<li><strong>自动化流程</strong>：从需求分析到代码生成、测试和部署，整个过程高度自动化。</li>
<li><strong>可扩展性</strong>：支持自定义角色和任务，适应不同的开发需求。</li>
</ul>
<h4 id="应用场景："><strong>应用场景：</strong></h4>
<ul>
<li>自动化代码生成</li>
<li>快速原型开发</li>
<li>教育和培训（模拟软件开发流程）</li>
</ul>
<hr>
<h3 id="2-MetaGPT"><strong>2. MetaGPT</strong></h3>
<p>MetaGPT 是一个多智能体框架，旨在通过多个智能体的协作生成高质量的内容或解决复杂问题。它结合了生成式 AI 和多智能体系统的优势，能够处理更复杂的任务。</p>
<h4 id="特点：-2"><strong>特点：</strong></h4>
<ul>
<li><strong>任务分解</strong>：将复杂任务分解为多个子任务，由不同的智能体分别处理。</li>
<li><strong>协作生成</strong>：智能体之间通过协作生成内容，确保输出的准确性和一致性。</li>
<li><strong>知识共享</strong>：智能体可以共享知识和上下文，提高整体效率。</li>
<li><strong>模块化设计</strong>：支持灵活的任务分配和智能体配置。</li>
</ul>
<h4 id="应用场景：-2"><strong>应用场景：</strong></h4>
<ul>
<li>内容生成（如文章、报告、代码）</li>
<li>复杂问题求解</li>
<li>多领域知识整合</li>
</ul>
<hr>
<h3 id="3-对比与总结"><strong>3. 对比与总结</strong></h3>
<table>
<thead>
<tr>
<th>特性</th>
<th>ChatDev</th>
<th>MetaGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>核心目标</strong></td>
<td>自动化软件开发</td>
<td>多领域任务协作与内容生成</td>
</tr>
<tr>
<td><strong>智能体角色</strong></td>
<td>软件开发相关角色（如开发、测试）</td>
<td>通用角色（可自定义）</td>
</tr>
<tr>
<td><strong>协作方式</strong></td>
<td>对话驱动</td>
<td>任务分解与协作生成</td>
</tr>
<tr>
<td><strong>应用领域</strong></td>
<td>软件开发</td>
<td>多领域（内容生成、问题求解等）</td>
</tr>
<tr>
<td><strong>可扩展性</strong></td>
<td>支持自定义角色</td>
<td>高度模块化，灵活配置</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="4-其他多智能体框架"><strong>4. 其他多智能体框架</strong></h3>
<p>除了 ChatDev 和 MetaGPT，还有一些其他多智能体框架值得关注：</p>
<ul>
<li><strong>AutoGPT</strong>：通过多智能体协作实现自动化任务执行。</li>
<li><strong>LangChain</strong>：支持多智能体协作的框架，专注于语言模型的应用。</li>
<li><strong>Camel</strong>：专注于多智能体对话和协作的研究框架。</li>
</ul>
<hr>
<h3 id="5-未来发展方向"><strong>5. 未来发展方向</strong></h3>
<p>多智能体框架的未来发展可能集中在以下方面：</p>
<ul>
<li><strong>更高效的协作机制</strong>：提高智能体之间的协作效率，减少冗余。</li>
<li><strong>领域适应性</strong>：增强框架在不同领域的通用性和适应性。</li>
<li><strong>人机协作</strong>：支持人类与智能体的无缝协作。</li>
<li><strong>伦理与安全</strong>：确保多智能体系统的决策和行为符合伦理和安全标准。</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>智能体</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>智能体</tag>
        <tag>多智能体</tag>
        <tag>ChatDev</tag>
        <tag>MetaGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>常用的Unicode图标或Emoji简码</title>
    <url>/2025/04/14/%E5%B8%B8%E7%94%A8%E7%9A%84Unicode%E5%9B%BE%E6%A0%87%E6%88%96Emoji%E7%AE%80%E7%A0%81/</url>
    <content><![CDATA[<p>在 Markdown 中，你可以使用 <strong>Unicode 字符</strong> 或 <strong>Emoji 简码</strong> 来插入各种常用图标。以下是一些常见的分类整理，适用于文档、链接、提示等场景：</p>
<hr>
<h3 id="📌-常用图标分类">📌 <strong>常用图标分类</strong></h3>
<span id="more"></span>
<h4 id="📂-文件-文件夹">📂 <strong>文件 &amp; 文件夹</strong></h4>
<table>
<thead>
<tr>
<th>图标</th>
<th>Unicode</th>
<th>说明</th>
<th>Emoji 简码（部分平台）</th>
</tr>
</thead>
<tbody>
<tr>
<td>📄</td>
<td><code>U+1F4C4</code></td>
<td>文档</td>
<td><code>:page_facing_up:</code></td>
</tr>
<tr>
<td>📁</td>
<td><code>U+1F4C1</code></td>
<td>文件夹</td>
<td><code>:file_folder:</code></td>
</tr>
<tr>
<td>📂</td>
<td><code>U+1F4C2</code></td>
<td>打开的文件夹</td>
<td><code>:open_file_folder:</code></td>
</tr>
<tr>
<td>📝</td>
<td><code>U+1F4DD</code></td>
<td>笔记/编辑</td>
<td><code>:memo:</code></td>
</tr>
<tr>
<td>📎</td>
<td><code>U+1F4CE</code></td>
<td>回形针（附件）</td>
<td><code>:paperclip:</code></td>
</tr>
<tr>
<td>📊</td>
<td><code>U+1F4CA</code></td>
<td>图表/统计</td>
<td><code>:bar_chart:</code></td>
</tr>
</tbody>
</table>
<h4 id="🔗-链接-操作">🔗 <strong>链接 &amp; 操作</strong></h4>
<p>| 🔗 | <code>U+1F517</code> | 超链接 | <code>:link:</code> |<br>
| ⬇️ | <code>U+2B07</code> | 下载 | <code>:arrow_down:</code> |<br>
| ↗️ | <code>U+2197</code> | 外部链接 | <code>:arrow_upper_right:</code> |<br>
| 🔍 | <code>U+1F50D</code> | 搜索 | <code>:mag:</code> |<br>
| ✏️ | <code>U+270F</code> | 编辑 | <code>:pencil2:</code> |</p>
<h4 id="⚠️-提示-状态">⚠️ <strong>提示 &amp; 状态</strong></h4>
<p>| ❗ | <code>U+2757</code> | 重要警告 | <code>:exclamation:</code> |<br>
| ✅ | <code>U+2705</code> | 完成/成功 | <code>:white_check_mark:</code> |<br>
| ❌ | <code>U+274C</code> | 错误/关闭 | <code>:x:</code> |<br>
| ⚠️ | <code>U+26A0</code> | 警告 | <code>:warning:</code> |<br>
| 🔥 | <code>U+1F525</code> | 紧急/热点 | <code>:fire:</code> |</p>
<h4 id="💻-技术相关">💻 <strong>技术相关</strong></h4>
<p>| 💾 | <code>U+1F4BE</code> | 磁盘/保存 | <code>:floppy_disk:</code> |<br>
| 📱 | <code>U+1F4F1</code> | 手机/移动端 | <code>:iphone:</code> |<br>
| 🌐 | <code>U+1F310</code> | 网页/全球 | <code>:globe_with_meridians:</code> |<br>
| 🛠️ | <code>U+1F6E0</code> | 工具/设置 | <code>:hammer_and_wrench:</code> |</p>
<h4 id="🕒-时间-日期">🕒 <strong>时间 &amp; 日期</strong></h4>
<p>| ⏰ | <code>U+23F0</code> | 闹钟/时间 | <code>:alarm_clock:</code> |<br>
| 📅 | <code>U+1F4C5</code> | 日历 | <code>:date:</code> |<br>
| ⌛ | <code>U+231B</code> | 等待/加载 | <code>:hourglass:</code> |</p>
<hr>
<h3 id="✨-使用示例">✨ <strong>使用示例</strong></h3>
<ol>
<li>
<p><strong>带图标的链接</strong></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">[<span class="string">📄 用户手册</span>](<span class="link">https://example.com/manual</span>)</span><br><span class="line">[<span class="string">⬇️ 下载最新版</span>](<span class="link">https://example.com/download</span>)</span><br></pre></td></tr></table></figure>
<p>效果：<br>
📄 <a href="#">用户手册</a> | ⬇️ <a href="#">下载最新版</a></p>
</li>
<li>
<p><strong>提示框</strong></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">⚠️ 注意：操作不可逆！</span><br><span class="line">✅ 已完成备份。</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>技术文档</strong></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">🛠️ 配置步骤：</span><br><span class="line"><span class="bullet">1.</span> 打开 <span class="code">`💾 settings.ini`</span></span><br><span class="line"><span class="bullet">2.</span> 修改 <span class="code">`🌐 proxy_url`</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h3 id="🔎-如何快速输入这些图标？">🔎 <strong>如何快速输入这些图标？</strong></h3>
<ol>
<li><strong>Windows/Linux</strong>：按 <code>Win + .</code> 或 <code>Win + ;</code> 打开表情符号面板。</li>
<li><strong>Mac</strong>：按 <code>Cmd + Ctrl + Space</code>。</li>
<li><strong>直接复制</strong>：从 <a href="https://unicode-table.com/">Unicode 图标大全</a> 或 <a href="https://emojipedia.org/">Emoji 列表</a> 中粘贴。</li>
</ol>
<p>如果需要更复杂的图标（如彩色），可能需要借助 HTML/CSS 或特定平台的扩展语法（如 GitHub 的 <code>:emoji:</code> 简码）。</p>
]]></content>
      <categories>
        <category>自媒体</category>
        <category>图标</category>
      </categories>
      <tags>
        <tag>自媒体</tag>
        <tag>图表</tag>
        <tag>表情</tag>
      </tags>
  </entry>
  <entry>
    <title>微信接入LLM或工作流</title>
    <url>/2025/06/04/%E5%BE%AE%E4%BF%A1%E6%8E%A5%E5%85%A5LLM%E6%88%96%E5%B7%A5%E4%BD%9C%E6%B5%81/</url>
    <content><![CDATA[<h3 id="1-WeChatPadPro-连接微信的工具">1. WeChatPadPro 连接微信的工具</h3>
<p>项目地址：<a href="https://github.com/WeChatPadPro/WeChatPadPro">https://github.com/WeChatPadPro/WeChatPadPro</a></p>
<p>WeChatPadPro部署文档：<a href="https://astrbot.app/deploy/platform/wechat/wechatpadpro.html">https://astrbot.app/deploy/platform/wechat/wechatpadpro.html</a></p>
<p>默认登陆url：<a href="http://localhost:38849">http://localhost:38849</a></p>
<span id="more"></span>
<h3 id="2-AstrBot-接入-WeChatPadPro-和-LLM及其应用">2. AstrBot 接入 WeChatPadPro 和 LLM及其应用</h3>
<p>项目地址：<a href="https://github.com/AstrBotDevs/AstrBot">https://github.com/AstrBotDevs/AstrBot</a></p>
<p>默认登陆url：<a href="http://localhost:6185">http://localhost:6185</a></p>
<h4 id="几个基础命令：">几个基础命令：</h4>
<ul>
<li>获取斜杠内置指令：/help</li>
<li>更换大模型或者工作流：/provider</li>
<li>查看好友或群成员 账号ID（可以添加为管理员）或者会话ID（可以添加为白名单）：/sid</li>
<li>详细说明可查看AstrBot平台的配置文件页面</li>
</ul>
<p>配置 WeChatPadPro 截图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604181410937.png" alt="image-20250604181410937"></p>
<p>以下是配置好后非第一次启动的日志，如果你第一次配置，保存后，需要按照图片说明扫码登陆。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604183736180.png" alt="image-20250604183736180"></p>
<p>配置 LLM 示例：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604181605046.png" alt="image-20250604181605046"></p>
<p>配置dify的chatflow工作流：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604184351532.png" alt="image-20250604184351532"></p>
<p>如果大家是本地部署，可能需要修改域名 localhost 为 host.docker.internal，这个自己看一下。（我是做了内网穿透的域名，不需要更改）</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604185343518.png" alt="image-20250604185343518"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250604185454772.png" alt="image-20250604185454772"></p>
<p>微信群对话示例：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/1.png" alt="1"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/2.png" alt="2"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/3.png" alt="3"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/4.png" alt="4"></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>Agent</category>
        <category>微信</category>
      </categories>
      <tags>
        <tag>微信</tag>
        <tag>dify</tag>
        <tag>wechatpadpro</tag>
        <tag>astrbot</tag>
        <tag>llm</tag>
      </tags>
  </entry>
  <entry>
    <title>升级 Dify</title>
    <url>/2025/03/19/%E5%8D%87%E7%BA%A7dify/</url>
    <content><![CDATA[<h4 id="将社区版迁移至-v1-x-x（0-15-x版本迁移至1-x-x版本）">将社区版迁移至 v1.x.x（0.15.x版本迁移至1.x.x版本）</h4>
<p>链接：<a href="https://docs.dify.ai/development/migration/migrate-to-v1">https://docs.dify.ai/development/migration/migrate-to-v1</a></p>
<h4 id="小版本升级参考如下：">小版本升级参考如下：</h4>
<ul>
<li>
<h5 id="进入dify源码的docker目录，执行以下命令：">进入dify源码的docker目录，执行以下命令：</h5>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd dify/docker</span><br><span class="line">cp docker-compose.yaml docker-compose.yaml.$(date +%s).bak  #备份compose文件（可选）</span><br><span class="line">docker compose down</span><br><span class="line">git checkout main</span><br><span class="line">git pull origin main</span><br><span class="line">tar -cvf volumes-$(date +%s).tgz volumes  # 备份数据（可选）</span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure>
<ul>
<li>
<h5 id="同步环境变量配置（重要）">同步环境变量配置（重要）</h5>
</li>
</ul>
<p>去到dify/docker目录下，查看.env.example文件的更新日期判断是否已更新</p>
<p>如果.env.example文件已更新，请确保相应地修改本地的.env文件。</p>
<p>根据需要检查并修改.env文件中的配置项，以确保它们与您的实际环境相匹配。您可能需要添加任何新变量从.env.example到.env文件中，并更新任何已更改的值。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>AI工具</category>
      </categories>
      <tags>
        <tag>dify</tag>
        <tag>rag</tag>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>快速上手MCP</title>
    <url>/2025/05/14/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BMCP/</url>
    <content><![CDATA[<h2 id="1-MCP入门介绍">1. MCP入门介绍</h2>
<ul>
<li>MCP，全称是Model Context Protocol，模型上下文协议，由Claude母公司Anthropic于去年11月正式提出。
<ul>
<li>Anthropic MCP发布通告：<a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a></li>
<li>MCP GitHub主页：<a href="https://github.com/modelcontextprotocol">https://github.com/modelcontextprotocol</a></li>
</ul>
</li>
</ul>
<span id="more"></span>
<ul>
<li>总的来说 <strong>MCP解决的最大痛点，就是Agent开发中调用外部工具的技术门槛过高的问题。</strong>
<ul>
<li>我们都知道，<strong>能调用外部工具，是大模型进化为智能体Agent的关键</strong>，如果不能使用外部工具，大模型就只能是个简单的聊天机器人，甚至连查询天气都做不到。由于底层技术限制啊，大模型本身是无法和外部工具直接通信的，因此 <strong>Function calling</strong> 的思路，就是创建一个外部函数（function）作为中介，一边传递大模型的请求，另一边调用外部工具，最终让大模型能够间接的调用外部工具。</li>
<li>MCP客户端对MCP服务器上的工具调用流程如下：
<ul>
<li><strong>Step 1. 建立和服务器的通信</strong>；</li>
<li><strong>Step 2. 查询服务器上总共有多少个外部工具；</strong></li>
<li><strong>Step 3. 将外部工具组成列表，带入到当前对话中；</strong></li>
<li><strong>Step 4. 借助Function calling进行外部工具调用。</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154429464.png" alt="image-20250515154429464"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154406481.png" alt="image-20250515154406481"></p>
<h4 id="MCP技术本质：Function-calling的更高层实现">MCP技术本质：Function calling的更高层实现</h4>
<p>​         而近一段时间大火的MCP技术，其实就可以将其理解为Function calling技术的更高层封装和实现。传统的Function calling技术要求围绕不同的外部工具API单独创建一个外部函数，类似一把锁单独配一把钥匙，而一个智能体又往往涉及到多个外部工具设计，因此开发工作量很大。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154333871.png" alt="image-20250515154333871"></p>
<p>而MCP技术，全称为Model Context Protocol，模型上下文协议，是一种开发者共同遵守的协议，在这个协议框架下，大家围绕某个API开发的外部工具就能够共用，从而大幅减少重复造轮子的时间。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154310284.png" alt="image-20250515154310284"></p>
<h2 id="2-MCP服务器集合">2. MCP服务器集合</h2>
<ul>
<li>MCP官方服务器合集：<a href="https://github.com/modelcontextprotocol/servers">https://github.com/modelcontextprotocol/servers</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154222173.png" alt="image-20250515154222173"></p>
<ul>
<li>MCP Github热门导航：<a href="https://github.com/punkpeye/awesome-mcp-servers">https://github.com/punkpeye/awesome-mcp-servers</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154154901.png" alt="image-20250515154154901"></p>
<ul>
<li>Smithery：<a href="https://smithery.ai/">https://smithery.ai/</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154128451.png" alt="image-20250515154128451"></p>
<ul>
<li>MCP导航：<a href="https://mcp.so/">https://mcp.so/</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154033905.png" alt="image-20250515154033905"></p>
<ul>
<li>Composio 托管的 MCP 服务器: <a href="https://mcp.composio.dev/">https://mcp.composio.dev/</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515154000165.png" alt="image-20250515154000165"></p>
<ul>
<li>Zapier 托管的 MCP 服务器: <a href="https://actions.zapier.com/settings/mcp">https://actions.zapier.com/settings/mcp</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515153934030.png" alt="image-20250515153934030"></p>
<ul>
<li>modelscope魔搭社区 mcp 广场：<a href="https://www.modelscope.cn/mcp">https://www.modelscope.cn/mcp</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515153903975.png" alt="image-20250515153903975"></p>
<h2 id="3-MCP目前热门客户端软件">3. MCP目前热门客户端软件</h2>
<ul>
<li>Cursor： <a href="https://www.cursor.com/cn">https://www.cursor.com/cn</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515153753652.png" alt="image-20250515153753652"></p>
<ul>
<li>Claude Desktop： <a href="https://claude.ai/download">https://claude.ai/download</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515153717742.png" alt="image-20250515153717742"></p>
<ul>
<li>Cherry Studio：<a href="https://www.cherry-ai.com/">https://www.cherry-ai.com/</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515153644985.png" alt="image-20250515153644985"></p>
<ul>
<li>Cline： <a href="https://cline.bot/">https://cline.bot/</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515152400387.png" alt="image-20250515152400387"></p>
<ul>
<li>dify中基于MCP策略的Agent或MCP插件工具</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250515152256956.png" alt="image-20250515152256956"></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>Agent</category>
        <category>MCP</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>AI</tag>
        <tag>Agent</tag>
        <tag>MCP</tag>
        <tag>Function Call</tag>
      </tags>
  </entry>
  <entry>
    <title>扩散模型和 自回归模型的对比</title>
    <url>/2025/03/06/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%92%8C-%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>扩散模型（Diffusion Models）和自回归模型（Autoregressive Models）是生成模型中的两种重要方法，广泛应用于图像、文本和音频生成任务。它们各有特点，适用于不同的场景。</p>
<hr>
<span id="more"></span>
<h3 id="1-扩散模型（Diffusion-Models）"><strong>1. 扩散模型（Diffusion Models）</strong></h3>
<p>扩散模型是一种基于概率的生成模型，其核心思想是通过逐步添加噪声将数据分布转化为简单分布（如高斯分布），然后学习如何逆向去噪以生成新数据。</p>
<h4 id="核心思想"><strong>核心思想</strong></h4>
<ol>
<li>
<p><strong>前向过程（Forward Process）</strong>：</p>
<ul>
<li>
<p>数据（如图像）通过逐步添加高斯噪声被破坏，最终变成一个纯噪声分布。</p>
</li>
<li>
<p>这个过程是固定的，通常定义为马尔可夫链，每一步都添加少量噪声。</p>
</li>
<li>
<p>数学上，前向过程可以表示为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>)</mo><mo>=</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">;</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>β</mi><mi>t</mi></msub><mi>I</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9838800000000001em;"></span><span class="strut bottom" style="height:1.24001em;vertical-align:-0.2561299999999999em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.09388000000000007em;"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size1">√</span></span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.05278em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.90388em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.05278em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中，(x_t) 是第 (t) 步的噪声数据，(\beta_t) 是噪声的方差。</p>
<p>其中，(x_t) 是第 (t) 步的噪声数据，(\beta_t) 是噪声的方差。</p>
</li>
</ul>
</li>
<li>
<p><strong>逆向过程（Reverse Process）</strong>：</p>
<ul>
<li>模型学习如何从噪声数据逐步去噪，恢复出原始数据分布。</li>
<li>逆向过程通常通过神经网络参数化，学习每一步的条件分布：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">p_\theta(x_{t-1} | x_t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p>
</li>
</ul>
</li>
</ol>
<ul>
<li>目标是最大化数据似然，通常通过变分推断优化。</li>
</ul>
<ol start="3">
<li><strong>训练目标</strong>：
<ul>
<li>扩散模型的训练目标是优化逆向过程的参数，使其能够准确地从噪声数据中恢复原始数据。</li>
<li>常用的损失函数是基于均方误差（MSE）的去噪目标。</li>
</ul>
</li>
</ol>
<h4 id="优点"><strong>优点</strong></h4>
<ul>
<li>生成质量高：扩散模型在图像生成任务中表现出色，生成的图像细节丰富。</li>
<li>训练稳定：相比于GANs，扩散模型的训练过程更加稳定。</li>
<li>可解释性强：前向和逆向过程具有清晰的数学定义。</li>
</ul>
<h4 id="缺点"><strong>缺点</strong></h4>
<ul>
<li>生成速度慢：由于需要逐步去噪，生成过程通常较慢。</li>
<li>计算成本高：训练和推理过程需要较多的计算资源。</li>
</ul>
<h4 id="应用"><strong>应用</strong></h4>
<ul>
<li>图像生成（如DALL·E 2、Stable Diffusion）</li>
<li>音频生成</li>
<li>数据去噪</li>
</ul>
<hr>
<h3 id="2-自回归模型（Autoregressive-Models）"><strong>2. 自回归模型（Autoregressive Models）</strong></h3>
<p>自回归模型是一种基于序列的生成模型，其核心思想是利用序列中前面的元素预测后面的元素。它假设当前数据点只依赖于之前的数据点。</p>
<h4 id="核心思想-2"><strong>核心思想</strong></h4>
<ol>
<li>
<p><strong>序列建模</strong>：</p>
<ul>
<li>数据被看作一个序列（如文本、音频或图像的像素序列）。</li>
<li>模型通过条件概率分布逐步生成序列中的每个元素：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mi>p</mi><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">p(x) = \prod_{t=1}^T p(x_t | x_{&lt;t})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em;"></span><span class="strut bottom" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1671129999999998em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∏</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mrel">&lt;</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p>
其中，(x_t) 是序列中的第 (t) 个元素，(x_{&lt;t}) 是之前的所有元素。</li>
</ul>
</li>
<li>
<p><strong>条件概率建模</strong>：</p>
<ul>
<li>使用神经网络（如RNN、LSTM、Transformer）建模条件概率分布 (p(x_t | x_{&lt;t}))。</li>
<li>例如，在文本生成中，模型根据前面的单词预测下一个单词。</li>
</ul>
</li>
<li>
<p><strong>训练目标</strong>：</p>
<ul>
<li>自回归模型的训练目标是最大化序列的似然函数，通常通过交叉熵损失优化。</li>
</ul>
</li>
</ol>
<h4 id="优点-2"><strong>优点</strong></h4>
<ul>
<li>灵活性高：可以建模任意长度的序列。</li>
<li>生成质量好：在文本生成和语音合成等任务中表现优异。</li>
<li>可扩展性强：结合Transformer等强大架构，能够处理长序列数据。</li>
</ul>
<h4 id="缺点-2"><strong>缺点</strong></h4>
<ul>
<li>生成速度慢：由于序列是逐步生成的，生成过程较慢。</li>
<li>长程依赖问题：早期的自回归模型（如RNN）难以捕捉长程依赖，但Transformer部分解决了这一问题。</li>
</ul>
<h4 id="应用-2"><strong>应用</strong></h4>
<ul>
<li>文本生成（如GPT系列）</li>
<li>语音合成（如WaveNet）</li>
<li>图像生成（如PixelRNN、PixelCNN）</li>
</ul>
<hr>
<h3 id="3-扩散模型-vs-自回归模型"><strong>3. 扩散模型 vs 自回归模型</strong></h3>
<table>
<thead>
<tr>
<th>特性</th>
<th>扩散模型</th>
<th>自回归模型</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>生成方式</strong></td>
<td>逐步去噪</td>
<td>逐步预测序列</td>
</tr>
<tr>
<td><strong>生成速度</strong></td>
<td>较慢</td>
<td>较慢</td>
</tr>
<tr>
<td><strong>训练稳定性</strong></td>
<td>高</td>
<td>中等（取决于架构）</td>
</tr>
<tr>
<td><strong>建模能力</strong></td>
<td>适合连续数据（如图像）</td>
<td>适合离散数据（如文本）</td>
</tr>
<tr>
<td><strong>计算成本</strong></td>
<td>高</td>
<td>中等</td>
</tr>
<tr>
<td><strong>应用领域</strong></td>
<td>图像生成、去噪</td>
<td>文本生成、语音合成</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="4-结合与改进"><strong>4. 结合与改进</strong></h3>
<p>近年来，研究者尝试结合扩散模型和自回归模型的优点。例如：</p>
<ul>
<li>在图像生成中，使用自回归模型生成低分辨率图像，再用扩散模型细化细节。</li>
<li>在文本生成中，使用扩散模型生成隐变量，再用自回归模型生成文本。</li>
</ul>
<p>这些方法旨在提高生成质量和效率，同时降低计算成本。</p>
<hr>
<p>总结来说，扩散模型和自回归模型各有优劣，适用于不同的任务。扩散模型在图像生成中表现突出，而自回归模型在文本生成中占据主导地位。随着研究的深入，两者的结合可能会推动生成模型的发展。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>AI</tag>
        <tag>生成模型方法</tag>
      </tags>
  </entry>
  <entry>
    <title>机场推荐（魔法上网，科学上网）</title>
    <url>/2025/03/05/%E7%BF%BB%E5%A2%99%E6%9C%BA%E5%9C%BA%E6%8E%A8%E8%8D%90%EF%BC%88%E9%AD%94%E6%B3%95%E4%B8%8A%E7%BD%91%EF%BC%89/</url>
    <content><![CDATA[<h2 id="暂时推荐这几个机场（在没有魔法的情况下网站可以登录）">暂时推荐这几个机场（在没有魔法的情况下网站可以登录）</h2>
<h4 id="1-费用自己看着选，贵的速率和稳定性都好一些">1. 费用自己看着选，贵的速率和稳定性都好一些</h4>
<h4 id="2-如果大家有好的机场也可以给我推荐一下（最好在没有魔法时可以登录）">2. 如果大家有好的机场也可以给我推荐一下（最好在没有魔法时可以登录）</h4>
<span id="more"></span>
<ul>
<li>
<h5 id="第一个-牡牛网：https-牧牛-com-auth-register-code-mdSU">第一个 牡牛网：<a href="https://xn--11xxa.com/auth/register?code=mdSU">https://牧牛.com/auth/register?code=mdSU</a></h5>
<ul>
<li>
<p>推荐安装网页底部的Clash软件</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250305165344389.png" alt="image-20250305165344389"></li>
</ul>
</li>
<li>
<p>进入页面有详细安装教程，按照步骤操作就行</p>
</li>
<li>
<p>资费：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250305170236304.png" alt="image-20250305170236304"></li>
</ul>
</li>
</ul>
</li>
<li>
<h5 id="第二个-GW云洞：https-yundong-东方科仪-com-register-code-cXXoEDoR">第二个 GW云洞：<a href="https://yundong.xn--xhq8sm16c5ls.com/#/register?code=cXXoEDoR">https://yundong.东方科仪.com/#/register?code=cXXoEDoR</a></h5>
<ul>
<li>
<p>官方优惠码：GWyundong 享受9折！（若失效，自行查看官网优惠）</p>
</li>
<li>
<p>资费：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250705081542660.png" alt="gw云洞"></li>
</ul>
</li>
<li>
<p>教程页面有详细说明，自行查阅</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250705082413462.png" alt="image-20250705082413462"></li>
</ul>
</li>
</ul>
</li>
<li>
<h5 id="第三个-NiceCloud-https-mojie-best-register-code-HPbP6BDt">第三个 NiceCloud: <a href="https://mojie.best/#/register?code=HPbP6BDt">https://mojie.best/#/register?code=HPbP6BDt</a></h5>
<ul>
<li>
<p>专属88折优惠券：NICEXL88Z</p>
</li>
<li>
<p>资费：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250711202402055.png" alt="image-20250711202402055"></li>
</ul>
</li>
</ul>
</li>
<li>
<h5 id="第四个-魔戒-https-mojie-best-register-code-HPbP6BDt">第四个 魔戒: <a href="https://mojie.best/#/register?code=HPbP6BDt">https://mojie.best/#/register?code=HPbP6BDt</a></h5>
<ul>
<li>
<p>只有流量套餐，节点延迟相对前面的机场更高</p>
</li>
<li>
<p>资费：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250711200309339.png" alt="image-20250711200309339"></li>
</ul>
</li>
</ul>
</li>
<li>
<h5 id="第五个-Radial-VPN-https-radialvpn-io-invite-z0m5v9">第五个 Radial VPN: <a href="https://radialvpn.io/?invite=z0m5v9">https://radialvpn.io/?invite=z0m5v9</a></h5>
<ul>
<li>
<p>下载官网软件按着操作就行，相对前面的机场，这个价格偏高</p>
</li>
<li>
<p>资费：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250305170103021.png" alt="image-20250305170103021"></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>翻墙机场,</tag>
        <tag>魔法上网</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云域名解析</title>
    <url>/2025/04/25/%E9%98%BF%E9%87%8C%E4%BA%91%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h4 id="申请到自己的域名后，解析到内网穿透的IP，实现本地服务的访问">申请到自己的域名后，解析到内网穿透的IP，实现本地服务的访问</h4>
<span id="more"></span>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425015708668.png" alt="image-20250425015708668"></p>
<h5 id="成功添加解析记录后，就可以使用域名-端口号来访问本地的服务了。">成功添加解析记录后，就可以使用域名+端口号来访问本地的服务了。</h5>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425020339516.png" alt="image-20250425020339516"></p>
<h5 id="如果想去除端口号，需要本地做nginx转发配置，nginx必须是80和443端口，-windows一般需要关闭占用80端口的IIS服务。">如果想去除端口号，需要本地做nginx转发配置，nginx必须是80和443端口， windows一般需要关闭占用80端口的IIS服务。</h5>
<h6 id="打开启用和关闭windows功能，取消internet-information-services前面的勾选">打开启用和关闭windows功能，取消internet information services前面的勾选</h6>
<p><img src="https://cdn.jsdelivr.net/gh/hfhfn/image_storage@img/img/image-20250425021040774.png" alt="image-20250425021040774"></p>
]]></content>
      <categories>
        <category>工具</category>
        <category>网络服务</category>
      </categories>
      <tags>
        <tag>域名解析</tag>
      </tags>
  </entry>
</search>
